[
    {
        "version": "15",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\nimport csv\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nquery_parts_of_speech_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.json'\ncsv_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/reconciliation_results.csv'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return {\n            \"error\": f\"Received status code {response.status_code}\",\n            \"response_text\": response.text\n        }\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_path):\n        print(f\"Query Parts of Speech file not found at {query_parts_of_speech_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech data\n    with open(query_parts_of_speech_path, 'r', encoding='utf-8') as file:\n        query_parts_of_speech = json.load(file)\n\n    # Create a mapping from \"Bulgarian\" field to \"Part of Speech\"\n    bulgarian_to_pos = {}\n    for entry in query_parts_of_speech:\n        # Split by commas if multiple variations exist\n        bulgarian_variations = [variant.strip() for variant in entry[\"Bulgarian\"].split(\",\")]\n        for variation in bulgarian_variations:\n            bulgarian_to_pos[variation] = entry[\"Part of Speech\"]\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            data = entry[\"data\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            revised_part_of_speech = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Check for \"Received status code 204\" and matching criteria\n            if isinstance(data, dict) and \"error\" in data and \"Received status code 204\" in data[\"error\"]:\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            # Modify the query by removing the cutoff string\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n\n                            # Re-run the query\n                            revised_data = fetch_and_save(revised_query)\n\n                            # Check if the revised query returned a valid result\n                            if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                                revised_part_of_speech = \"Verb\"\n                            break  # Stop checking other cutoff strings for this query\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                        # Re-run the query\n                        revised_data = fetch_and_save(revised_query)\n\n                        # Check if the revised query matches Part of Speech = Adjective\n                        if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                            revised_part_of_speech = \"Adjective\"\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech, revised_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Received status code 204\", revised_status, revised_result\n                ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "44",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load concatenated data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # First Pass: Initial processing of queries\n    results = []\n    revised_queries_to_search = []  # Collect revised queries for the second pass\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        revised_status = \"\"\n        revised_result = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    revised_queries_to_search.append(revised_query)\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                revised_queries_to_search.append(revised_query)\n\n        # Determine \"Found in\" values\n        found_in_concatenated = (\n            \"Original\" if query in concatenated_queries else\n            \"Revised\" if revised_query in concatenated_queries else\n            \"Neither\"\n        )\n        found_in_pos = (\n            \"Original\" if query in bulgarian_to_pos else\n            \"Revised\" if revised_query in bulgarian_to_pos else\n            \"Neither\"\n        )\n\n        results.append([\n            query, revised_query, original_part_of_speech,\n            cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n            found_in_concatenated, found_in_pos\n        ])\n\n    # Second Pass: Process revised queries\n    for result in results:\n        revised_query = result[1]  # Revised Query\n        if revised_query:  # Only process if there's a revised query\n            found_in_concatenated = (\n                \"Original\" if revised_query in concatenated_queries else \"Neither\"\n            )\n            found_in_pos = (\n                \"Original\" if revised_query in bulgarian_to_pos else \"Neither\"\n            )\n            result[6] = \"Success\" if found_in_concatenated == \"Original\" else \"Failure\"  # Update Revised Status\n            result[7] = \"Match Found\" if found_in_pos == \"Original\" else \"No Match\"  # Update Revised Result\n\n    # Generate XLSX file\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"\n\n    # Write headers\n    headers = [\n        \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n        \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n        \"Found in concatenated\", \"Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Write data\n    for row in results:\n        sheet.append(row)\n\n    # Auto-fit column widths\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter\n        sheet.column_dimensions[column_letter].width = max_length + 2\n\n    # Apply table style\n    table_range = f\"A1:J{sheet.max_row}\"\n    table = Table(displayName=\"Table1\", ref=table_range)\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n        showRowStripes=True, showColumnStripes=False\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "70",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nprocessed_file_path = os.path.join(base_directory, \"processed.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\nschematized_file_path = os.path.join(base_directory, \"schematized.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef process_entries():\n    \"\"\"\n    Function to process the concatenated.json file into processed.json.\n    This step validates and enriches the data for subsequent reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Write processed data to processed.json\n        with open(processed_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(processed_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Processing completed. Processed data saved to {processed_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during processing: {e}\")\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the processed.json file and the Anki flashcards table.\n    \"\"\"\n    if not os.path.exists(processed_file_path):\n        print(f\"Processed JSON file not found at {processed_file_path}. Please run 'process' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load processed data\n        with open(processed_file_path, 'r', encoding='utf-8') as file:\n            processed_data = json.load(file)\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Yes\" if is_valid else \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\ndef schematize_entries():\n    \"\"\"\n    Function to schematize the processed.json data into a structured JSON schema.\n    \"\"\"\n    if not os.path.exists(processed_file_path):\n        print(f\"Processed JSON file not found at {processed_file_path}. Please run 'process' mode first.\")\n        return\n\n    try:\n        with open(processed_file_path, 'r', encoding='utf-8') as file:\n            processed_data = json.load(file)\n\n        schematized_data = []\n        for entry in processed_data:\n            # Create a structured schema for each entry\n            schematized_entry = {\n                \"query\": entry[\"query\"],\n                \"validity\": \"Valid\" if entry[\"is_valid\"] else \"Invalid\"\n            }\n            schematized_data.append(schematized_entry)\n\n        # Write schematized data to schematized.json\n        with open(schematized_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(schematized_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Schematization completed. Schematized data saved to {schematized_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during schematization: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelif mode == \"schematize\":\n    schematize_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, reconcile, concatenate, schematize\")"
    },
    {
        "version": "21",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"fetch\"  # Change this value to \"fetch\", \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nschema_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/schema.json'\nquery_parts_of_speech_csv_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.csv'\ncsv_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/reconciliation_results.csv'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_entries():\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")  # Updated behavior for fetch mode\n\n# Function to process individual JSON files\ndef process_entries():\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n            # Example processing logic\n            # Add your specific processing logic here\n            processed_data = {\n                \"term\": filename.replace(\".json\", \"\"),\n                \"response\": data\n            }\n\n            # Save the processed data back to the file\n            with open(file_path, 'w', encoding='utf-8') as file:\n                json.dump(processed_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n                concatenated_data.append(data)\n\n    with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n        json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    with open(schema_file_path, 'w', encoding='utf-8') as file:\n        json.dump(schema, file, indent=4, ensure_ascii=False)\n\n    print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n            \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            original_data = entry.get(\"data\", None)\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n\n                        # Check if the revised query exists in the concatenated data\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                        break  # Stop checking other cutoff strings for this query\n\n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                    # Check if the revised query exists in the concatenated data\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n\n            # Write the row to the CSV file\n            csvwriter.writerow([\n                query, revised_query, original_part_of_speech,\n                cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n            ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "31",
        "content": "# CSV Loading in reconcile_entries()\ntry:\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n    logging.debug(f\"Total mappings in Query Parts of Speech.csv: {len(bulgarian_to_pos)}\")\n    logging.debug(f\"Sample mappings from Query Parts of Speech.csv: {list(bulgarian_to_pos.items())[:5]}\")\nexcept (IOError, UnicodeDecodeError) as e:\n    logging.error(f\"Error reading CSV file '{query_parts_of_speech_csv_path}': {e}\")\n    return"
    },
    {
        "version": "60",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Create a mapping of queries to parts of speech and Note IDs\n        query_to_pos = {}\n        query_to_note_id = {}\n        for row in anki_data:\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n\n            if bulgarian_1:\n                query_to_pos[bulgarian_1] = part_of_speech\n                query_to_note_id[bulgarian_1] = note_id\n            if bulgarian_2:\n                query_to_pos[bulgarian_2] = part_of_speech\n                query_to_note_id[bulgarian_2] = note_id\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in query_to_pos\n            revised_found_in_pos = revised_query in query_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos, original_note_id, revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\", \"Original Note ID\", \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:L{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "54",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\",\n    \"\u0430\u043c\", \"\u044f\u043c\", \"\u0430\u0445\", \"\u044f\u0445\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n        \n        # Load the Query Parts of Speech JSON file\n        bulgarian_to_pos = {}\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n            if original_part_of_speech == \"Unknown\":\n                logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in bulgarian_to_pos\n            revised_found_in_pos = revised_query in bulgarian_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos\n            ])\n\n        # Log unmatched queries\n        if unmatched_revised_queries:\n            logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n            for qr in unmatched_revised_queries:\n                logging.warning(f\"Unmatched Revised Query: {qr}\")\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:J{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during processing: {e}\")\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "05",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\"\nmode = \"fetch\"  # Change this value to \"process\" or \"concatenate\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated_minified.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Function to concatenate all JSON files into a single minified JSON file\ndef concatenate_json_files_minified():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a single minified JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, separators=(',', ':'))\n\n    print(f\"Minified concatenated JSON data saved to {concatenated_file_path}\")\n\n# Main workflow\nif mode == \"fetch\":\n    with open(input_file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            query_term = line.strip()\n            if query_term:\n                fetch_and_save(query_term)\n    print(\"Fetching and saving completed.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_minified()\n    print(\"Concatenation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate\")"
    },
    {
        "version": "25",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"locked\", \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"locked\"  # Default mode is \"locked\" to prevent accidental usage\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function for locked mode\ndef locked_mode():\n    print(\"The script is in 'locked' mode. No operations will be performed.\")\n\n# Function to fetch data from the API\ndef fetch_entries():\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")  # Updated behavior for fetch mode\n\n# Function to process individual JSON files\ndef process_entries():\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n            # Example processing logic\n            # Add your specific processing logic here\n            processed_data = {\n                \"term\": filename.replace(\".json\", \"\"),\n                \"response\": data\n            }\n\n            # Save the processed data back to the file\n            with open(file_path, 'w', encoding='utf-8') as file:\n                json.dump(processed_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n                concatenated_data.append(data)\n\n    with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n        json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    with open(schema_file_path, 'w', encoding='utf-8') as file:\n        json.dump(schema, file, indent=4, ensure_ascii=False)\n\n    print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n            \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            original_data = entry.get(\"data\", None)\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n\n                        # Check if the revised query exists in the concatenated data\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                        break  # Stop checking other cutoff strings for this query\n\n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                    # Check if the revised query exists in the concatenated data\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n\n            # Write the row to the CSV file\n            csvwriter.writerow([\n                query, revised_query, original_part_of_speech,\n                cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n            ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"locked\":\n    locked_mode()\nelif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: locked, fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "74",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\nschematized_file_path = os.path.join(base_directory, \"schematized.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the concatenated.json file and the Anki flashcards table.\n    This function also processes the concatenated.json file before reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load and process concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Check if the entry is \"Found, no headword\"\n                found_no_headword = False\n                if isinstance(data, list):\n                    for item in data:\n                        hits = item.get(\"hits\", [])\n                        # Check if there is no hit with type \"entry\"\n                        if not any(hit.get(\"type\") == \"entry\" for hit in hits):\n                            found_no_headword = True\n                            break\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid,\n                    \"found_no_headword\": found_no_headword\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n            found_no_headword = entry[\"found_no_headword\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Found, no headword\" if found_no_headword else (\"Yes\" if is_valid else \"No\")\n            revised_found_in_concatenated = \"Found, no headword\" if found_no_headword else \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\ndef schematize_entries():\n    \"\"\"\n    Function to schematize the concatenated.json data into a structured JSON schema.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        schematized_data = []\n        for entry in concatenated_data:\n            query = entry.get(\"query\", \"\")\n            data = entry.get(\"data\", {})\n\n            # Create a structured schema for each entry\n            schematized_entry = {\n                \"query\": query,\n                \"response_text\": data.get(\"response_text\", \"\"),\n                \"error\": data.get(\"error\", None)\n            }\n            schematized_data.append(schematized_entry)\n\n        # Write schematized data to schematized.json\n        with open(schematized_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(schematized_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Schematization completed. Schematized data saved to {schematized_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during schematization: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelif mode == \"schematize\":\n    schematize_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, reconcile, concatenate, schematize\")"
    },
    {
        "version": "40",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.json: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Create a new Excel workbook\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Reconciliation Results\"\n\n    # Write the headers\n    headers = [\n        \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n        \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n        \"Found in concatenated\", \"Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Iterate over the concatenated data to find entries matching the criteria\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        revised_status = \"\"\n        revised_result = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                if revised_entry and revised_entry.get(\"data\"):\n                    revised_status = \"Success\"\n                    revised_result = \"Match Found\"\n\n        # Determine \"Found in\" values\n        found_in_concatenated = (\n            \"Original\" if query in concatenated_queries else\n            \"Revised\" if revised_query in concatenated_queries else\n            \"Neither\"\n        )\n        found_in_pos = (\n            \"Original\" if query in bulgarian_to_pos else\n            \"Revised\" if revised_query in bulgarian_to_pos else\n            \"Neither\"\n        )\n\n        # Leave \"Revised Status\" and \"Revised Result\" blank if \"Found in POS\" equals \"Original\"\n        if found_in_pos == \"Original\":\n            revised_status = \"\"\n            revised_result = \"\"\n\n        # Append the row to the worksheet\n        sheet.append([\n            query, revised_query, original_part_of_speech,\n            cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n            found_in_concatenated, found_in_pos\n        ])\n\n    # Create a table in the worksheet\n    table_range = f\"A1:J{sheet.max_row}\"  # Covers all rows including headers\n    table = Table(displayName=\"Table1\", ref=table_range)\n\n    # Apply a table style\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\",  # Dark Teal, Table Style Medium 2\n        showFirstColumn=False,\n        showLastColumn=False,\n        showRowStripes=True,\n        showColumnStripes=True\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook to an XLSX file\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "11",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nschema_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/schema.json'\nquery_parts_of_speech_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch data from the API\ndef fetch_and_save(term, output_file_path):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_path):\n        print(f\"Query Parts of Speech file not found at {query_parts_of_speech_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech data\n    with open(query_parts_of_speech_path, 'r', encoding='utf-8') as file:\n        query_parts_of_speech = json.load(file)\n\n    # Create a mapping from \"Bulgarian\" field to \"Part of Speech\"\n    bulgarian_to_pos = {entry[\"Bulgarian\"]: entry[\"Part of Speech\"] for entry in query_parts_of_speech}\n\n    # Iterate over the concatenated data to find entries matching the criteria\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        data = entry[\"data\"]\n\n        # Check for \"Received status code 204\" and matching criteria\n        if isinstance(data, dict) and \"error\" in data and \"Received status code 204\" in data[\"error\"]:\n            if query in bulgarian_to_pos and bulgarian_to_pos[query] == \"Verb\":\n                if query.endswith(\" \u0441\u0435\") or query.endswith(\" (\u0441\u0435)\"):\n                    # Modify the query by removing the \" \u0441\u0435\" or \" (\u0441\u0435)\"\n                    revised_query = re.sub(r\"( \u0441\u0435| \\(\u0441\u0435\\))$\", \"\", query)\n\n                    # Re-run the query\n                    output_file_path = os.path.join(output_directory, f\"{revised_query}.json\")\n                    fetch_and_save(revised_query, output_file_path)\n\n                    # Check if the new query returned a valid result\n                    with open(output_file_path, 'r', encoding='utf-8') as revised_file:\n                        revised_data = json.load(revised_file)\n\n                        if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                            print(f\"Revised query '{revised_query}' for original query '{query}' succeeded.\")\n                        else:\n                            print(f\"Revised query '{revised_query}' for original query '{query}' still failed.\")\n\n    print(\"Reconciliation completed.\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "01",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Main workflow: Fetch and process\n# Step 1: Read query terms from the input file and fetch data\nwith open(input_file_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        query_term = line.strip()\n        if query_term:\n            fetch_and_save(query_term)\n\n# Step 2: Process the saved JSON files\nfor filename in os.listdir(output_directory):\n    if filename.endswith('.json'):\n        process_json_file(os.path.join(output_directory, filename))"
    },
    {
        "version": "50",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load concatenated data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # First Pass: Initial processing of queries\n    results = []\n    unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        original_found_in_concatenated = \"No\"\n        original_found_in_pos = \"No\"\n        revised_status = \"\"\n        revised_result = \"\"\n        revised_part_of_speech = \"\"\n        revised_found_in_concatenated = \"\"\n        revised_found_in_pos = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u0435\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u0435\u043d\u043e\"):\n                revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n\n        # Determine \"Found in\" values for original query\n        original_found_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n        original_found_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n\n        # Validate Revised Query\n        if revised_query:\n            revised_found_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n            revised_found_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n            revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n            revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n            revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n            # Log unmatched revised queries\n            if revised_found_in_concatenated == \"No\":\n                unmatched_revised_queries.append(revised_query)\n        else:\n            # Ensure blanks for revised fields if revised query is blank\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_found_in_concatenated = \"\"\n            revised_found_in_pos = \"\"\n\n        results.append([\n            query, original_part_of_speech, original_found_in_concatenated, original_found_in_pos,\n            revised_query, revised_part_of_speech, cutoff_type, cutoff_applied, revised_status, revised_result,\n            revised_found_in_concatenated, revised_found_in_pos\n        ])\n\n    # Log unmatched queries\n    if unmatched_revised_queries:\n        logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n        for qr in unmatched_revised_queries:\n            logging.warning(f\"Unmatched Revised Query: {qr}\")\n\n    # Generate XLSX file\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"\n\n    # Write headers\n    headers = [\n        \"Original Query\", \"Original Part of Speech\", \"Original Found in Concatenated\", \"Original Found in POS\",\n        \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n        \"Revised Status\", \"Revised Result\", \"Revised Found in Concatenated\", \"Revised Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Write data\n    for row in results:\n        sheet.append(row)\n\n    # Auto-fit column widths\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter\n        sheet.column_dimensions[column_letter].width = max_length + 2\n\n    # Apply table style\n    table_range = f\"A1:L{sheet.max_row}\"\n    table = Table(displayName=\"Table1\", ref=table_range)\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n        showRowStripes=True, showColumnStripes=False\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "64",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            if query in concatenated_queries:\n                original_found_in_concatenated = \"Yes\"\n            if revised_query:\n                if revised_query in concatenated_queries:\n                    revised_found_in_concatenated = \"Yes\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "35",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # Construct the API URL for the term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n            summary_data[f\"Failed API fetch for: {term}\"] += 1\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n            summary_data[f\"File write error for: {term}\"] += 1\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.json: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Open the CSV file for writing\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                    summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "24",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"locked\", \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"locked\"  # Default mode is \"locked\" to prevent accidental usage\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function for locked mode\ndef locked_mode():\n    print(\"The script is in 'locked' mode. No operations will be performed.\")\n\n# Function to fetch data from the API\ndef fetch_entries():\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")  # Updated behavior for fetch mode\n\n# Function to process individual JSON files\ndef process_entries():\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n            # Example processing logic\n            # Add your specific processing logic here\n            processed_data = {\n                \"term\": filename.replace(\".json\", \"\"),\n                \"response\": data\n            }\n\n            # Save the processed data back to the file\n            with open(file_path, 'w', encoding='utf-8') as file:\n                json.dump(processed_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n                concatenated_data.append(data)\n\n    with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n        json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    with open(schema_file_path, 'w', encoding='utf-8') as file:\n        json.dump(schema, file, indent=4, ensure_ascii=False)\n\n    print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n            \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            original_data = entry.get(\"data\", None)\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n\n                        # Check if the revised query exists in the concatenated data\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                        break  # Stop checking other cutoff strings for this query\n\n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                    # Check if the revised query exists in the concatenated data\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n\n            # Write the row to the CSV file\n            csvwriter.writerow([\n                query, revised_query, original_part_of_speech,\n                cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n            ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"locked\":\n    locked_mode()\nelif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: locked, fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "75",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the concatenated.json file and the Anki flashcards table.\n    This function also processes the concatenated.json file before reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load and process concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Check if the entry is \"Found, no headword\"\n                found_no_headword = False\n                if isinstance(data, list):\n                    for item in data:\n                        hits = item.get(\"hits\", [])\n                        # Check if there is no hit with type \"entry\"\n                        if not any(hit.get(\"type\") == \"entry\" for hit in hits):\n                            found_no_headword = True\n                            break\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid,\n                    \"found_no_headword\": found_no_headword\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n            found_no_headword = entry[\"found_no_headword\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Found, no headword\" if found_no_headword else (\"Yes\" if is_valid else \"No\")\n            revised_found_in_concatenated = \"Found, no headword\" if found_no_headword else \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, reconcile, concatenate\")"
    },
    {
        "version": "41",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.json: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Create a new Excel workbook\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"  # Changed the sheet title to \"Sheet1\"\n\n    # Write the headers\n    headers = [\n        \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n        \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n        \"Found in concatenated\", \"Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Iterate over the concatenated data to find entries matching the criteria\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        revised_status = \"\"\n        revised_result = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                if revised_entry and revised_entry.get(\"data\"):\n                    revised_status = \"Success\"\n                    revised_result = \"Match Found\"\n\n        # Determine \"Found in\" values\n        found_in_concatenated = (\n            \"Original\" if query in concatenated_queries else\n            \"Revised\" if revised_query in concatenated_queries else\n            \"Neither\"\n        )\n        found_in_pos = (\n            \"Original\" if query in bulgarian_to_pos else\n            \"Revised\" if revised_query in bulgarian_to_pos else\n            \"Neither\"\n        )\n\n        # Leave \"Revised Status\" and \"Revised Result\" blank if \"Found in POS\" equals \"Original\"\n        if found_in_pos == \"Original\":\n            revised_status = \"\"\n            revised_result = \"\"\n\n        # Append the row to the worksheet\n        sheet.append([\n            query, revised_query, original_part_of_speech,\n            cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n            found_in_concatenated, found_in_pos\n        ])\n\n    # Create a table in the worksheet\n    table_range = f\"A1:J{sheet.max_row}\"  # Covers all rows including headers\n    table = Table(displayName=\"Table1\", ref=table_range)\n\n    # Apply a table style\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\",  # Dark Teal, Table Style Medium 2\n        showFirstColumn=False,\n        showLastColumn=False,\n        showRowStripes=True,\n        showColumnStripes=True\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook to an XLSX file\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "10",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\"\nmode = \"schematize\"  # Change this value to \"process\", \"concatenate\", or \"schematize\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nschema_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/schema.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\n# NOTE: This function has been commented out to prevent accidental usage\n'''\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n'''\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Function to concatenate all JSON files into a single pretty-printed JSON file\ndef concatenate_json_files_pretty():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a pretty-printed JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Pretty-printed concatenated JSON data saved to {concatenated_file_path}\")\n\n# Function to generate a data dictionary (schema) from the concatenated JSON file\ndef generate_schema():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        data = json.load(file)\n\n    schema = {}\n\n    def explore_structure(obj, path=\"\"):\n        \"\"\"Recursively explore the structure of an object to generate a schema.\"\"\"\n        if isinstance(obj, dict):\n            for key, value in obj.items():\n                current_path = f\"{path}.{key}\" if path else key\n                explore_structure(value, current_path)\n        elif isinstance(obj, list):\n            if path not in schema:\n                schema[path] = {\"type\": \"list\", \"examples\": []}\n            for item in obj[:5]:  # Limit to 5 examples to keep the schema concise\n                explore_structure(item, path)\n        else:\n            if path not in schema:\n                schema[path] = {\"type\": type(obj).__name__, \"examples\": []}\n            if obj not in schema[path][\"examples\"] and len(schema[path][\"examples\"]) < 5:\n                schema[path][\"examples\"].append(obj)\n\n    # Explore the structure of the concatenated JSON data\n    explore_structure(data)\n\n    # Save the schema to a JSON file\n    with open(schema_output_file, 'w', encoding='utf-8') as out_file:\n        json.dump(schema, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Schema generated and saved to {schema_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize\")"
    },
    {
        "version": "51",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n        \n        # Load the Query Parts of Speech JSON file\n        bulgarian_to_pos = {}\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"No\"\n            original_found_in_pos = \"No\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            revised_found_in_concatenated = \"\"\n            revised_found_in_pos = \"\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n            if original_part_of_speech == \"Unknown\":\n                logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words ending in \"\u0435\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original query\n            original_found_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n            original_found_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_found_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n                revised_found_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n                revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            results.append([\n                query, original_part_of_speech, original_found_in_concatenated, original_found_in_pos,\n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied, revised_status, revised_result,\n                revised_found_in_concatenated, revised_found_in_pos\n            ])\n\n        # Log unmatched queries\n        if unmatched_revised_queries:\n            logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n            for qr in unmatched_revised_queries:\n                logging.warning(f\"Unmatched Revised Query: {qr}\")\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Original Found in Concatenated\", \"Original Found in POS\",\n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n            \"Revised Status\", \"Revised Result\", \"Revised Found in Concatenated\", \"Revised Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:L{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during processing: {e}\")\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "65",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = {}  # Dictionary to track query status in concatenated data\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                data = entry.get(\"data\", {})\n                # Mark as \"No\" if there's an error or empty response_text\n                if data.get(\"error\") or not data.get(\"response_text\", \"\").strip():\n                    concatenated_queries[query] = \"No\"\n                else:\n                    concatenated_queries[query] = \"Yes\"\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Determine \"Found in Concatenated\" for the original query\n            original_found_in_concatenated = concatenated_queries.get(query, \"No\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in Concatenated\" and \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = concatenated_queries.get(revised_query, \"No\")\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "34",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # Construct the API URL for the term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n            summary_data[f\"Failed API fetch for: {term}\"] += 1\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n            summary_data[f\"File write error for: {term}\"] += 1\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        logging.error(f\"Query Parts of Speech CSV file not found at: {query_parts_of_speech_csv_path}\")\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n            csv_reader = csv.DictReader(file)\n            for row in csv_reader:\n                bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.csv: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.csv: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading CSV file '{query_parts_of_speech_csv_path}': {e}\")\n        return\n\n    # Open the CSV file for writing\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                    summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "14",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nschema_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/schema.json'\nquery_parts_of_speech_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_and_save(term, output_file_path):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_path):\n        print(f\"Query Parts of Speech file not found at {query_parts_of_speech_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech data\n    with open(query_parts_of_speech_path, 'r', encoding='utf-8') as file:\n        query_parts_of_speech = json.load(file)\n\n    # Create a mapping from \"Bulgarian\" field to \"Part of Speech\"\n    bulgarian_to_pos = {}\n    for entry in query_parts_of_speech:\n        # Split by commas if multiple variations exist\n        bulgarian_variations = [variant.strip() for variant in entry[\"Bulgarian\"].split(\",\")]\n        for variation in bulgarian_variations:\n            bulgarian_to_pos[variation] = entry[\"Part of Speech\"]\n\n    # Iterate over the concatenated data to find entries matching the criteria\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        data = entry[\"data\"]\n\n        # Check for \"Received status code 204\" and matching criteria\n        if isinstance(data, dict) and \"error\" in data and \"Received status code 204\" in data[\"error\"]:\n            # Handle Verbs with cutoff logic\n            if query in bulgarian_to_pos and bulgarian_to_pos[query] == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n\n                        # Re-run the query\n                        output_file_path = os.path.join(output_directory, f\"{revised_query}.json\")\n                        fetch_and_save(revised_query, output_file_path)\n\n                        # Check if the new query returned a valid result\n                        with open(output_file_path, 'r', encoding='utf-8') as revised_file:\n                            revised_data = json.load(revised_file)\n\n                            if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                                print(f\"Revised query '{revised_query}' for original query '{query}' succeeded.\")\n                            else:\n                                print(f\"Revised query '{revised_query}' for original query '{query}' still failed.\")\n                        break  # Stop checking other cutoff strings for this query\n            \n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif query in bulgarian_to_pos and bulgarian_to_pos[query] in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n\n                    # Re-run the query\n                    output_file_path = os.path.join(output_directory, f\"{revised_query}.json\")\n                    fetch_and_save(revised_query, output_file_path)\n\n                    # Check if the new query matches Part of Speech = Adjective\n                    with open(output_file_path, 'r', encoding='utf-8') as revised_file:\n                        revised_data = json.load(revised_file)\n\n                        if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                            print(f\"Revised query '{revised_query}' for original query '{query}' succeeded as Adjective.\")\n                        else:\n                            print(f\"Revised query '{revised_query}' for original query '{query}' still failed.\")\n\n    print(\"Reconciliation completed.\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "45",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load concatenated data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # First Pass: Initial processing of queries\n    results = []\n    revised_queries_to_search = []  # Collect revised queries for the second pass\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        revised_status = \"\"\n        revised_result = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    revised_queries_to_search.append(revised_query)\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                revised_queries_to_search.append(revised_query)\n\n        # Determine \"Found in\" values\n        found_in_concatenated = (\n            \"Original\" if query in concatenated_queries else\n            \"Revised\" if revised_query in concatenated_queries else\n            \"Neither\"\n        )\n        found_in_pos = (\n            \"Original\" if query in bulgarian_to_pos else\n            \"Revised\" if revised_query in bulgarian_to_pos else\n            \"Neither\"\n        )\n\n        results.append([\n            query, revised_query, original_part_of_speech,\n            cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n            found_in_concatenated, found_in_pos\n        ])\n\n    # Second Pass: Process revised queries\n    for result in results:\n        revised_query = result[1]  # Revised Query\n        if revised_query:  # Only process if there's a revised query\n            found_in_concatenated = (\n                \"Original\" if revised_query in concatenated_queries else \"Neither\"\n            )\n            found_in_pos = (\n                \"Revised\" if revised_query in bulgarian_to_pos else \"Neither\"\n            )\n            result[6] = \"Success\" if found_in_concatenated == \"Original\" else \"Failure\"  # Update Revised Status\n            result[7] = \"Match Found\" if found_in_pos == \"Revised\" else \"No Match\"  # Update Revised Result\n            result[9] = found_in_pos  # Update \"Found in POS\" to \"Revised\" if applicable\n\n    # Generate XLSX file\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"\n\n    # Write headers\n    headers = [\n        \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n        \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n        \"Found in concatenated\", \"Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Write data\n    for row in results:\n        sheet.append(row)\n\n    # Auto-fit column widths\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter\n        sheet.column_dimensions[column_letter].width = max_length + 2\n\n    # Apply table style\n    table_range = f\"A1:J{sheet.max_row}\"\n    table = Table(displayName=\"Table1\", ref=table_range)\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n        showRowStripes=True, showColumnStripes=False\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "71",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nprocessed_file_path = os.path.join(base_directory, \"processed.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\nschematized_file_path = os.path.join(base_directory, \"schematized.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef process_entries():\n    \"\"\"\n    Function to process the concatenated.json file into processed.json.\n    This step validates and enriches the data for subsequent reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Write processed data to processed.json\n        with open(processed_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(processed_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Processing completed. Processed data saved to {processed_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during processing: {e}\")\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the processed.json file and the Anki flashcards table.\n    \"\"\"\n    if not os.path.exists(processed_file_path):\n        print(f\"Processed JSON file not found at {processed_file_path}. Please run 'process' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load processed data\n        with open(processed_file_path, 'r', encoding='utf-8') as file:\n            processed_data = json.load(file)\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Yes\" if is_valid else \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\ndef schematize_entries():\n    \"\"\"\n    Function to schematize the concatenated.json data into a structured JSON schema.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        schematized_data = []\n        for entry in concatenated_data:\n            query = entry.get(\"query\", \"\")\n            data = entry.get(\"data\", {})\n\n            # Create a structured schema for each entry\n            schematized_entry = {\n                \"query\": query,\n                \"response_text\": data.get(\"response_text\", \"\"),\n                \"error\": data.get(\"error\", None)\n            }\n            schematized_data.append(schematized_entry)\n\n        # Write schematized data to schematized.json\n        with open(schematized_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(schematized_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Schematization completed. Schematized data saved to {schematized_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during schematization: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelif mode == \"schematize\":\n    schematize_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, reconcile, concatenate, schematize\")"
    },
    {
        "version": "20",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"fetch\", \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nschema_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/schema.json'\nquery_parts_of_speech_csv_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.csv'\ncsv_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/reconciliation_results.csv'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_entries():\n    if not os.path.exists(input_file_path):\n        print(f\"Input file not found at {input_file_path}. Please provide a valid input file.\")\n        return\n\n    # Read the input terms\n    with open(input_file_path, 'r', encoding='utf-8') as file:\n        terms = [line.strip() for line in file if line.strip()]\n\n    # Fetch data from the API for each term\n    for term in terms:\n        url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n        headers = {\n            \"X-Secret\": \"XXX\"\n        }\n        response = requests.get(url, headers=headers)\n\n        # Save the response to a JSON file\n        output_file_path = os.path.join(output_directory, f\"{term}.json\")\n        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n            if response.status_code == 200:\n                json.dump(response.json(), output_file, indent=4, ensure_ascii=False)\n            else:\n                json.dump({\n                    \"error\": f\"Received status code {response.status_code}\",\n                    \"response_text\": response.text\n                }, output_file, indent=4, ensure_ascii=False)\n\n    print(f\"Fetch completed. Results saved to {output_directory}\")\n\n# Function to process individual JSON files\ndef process_entries():\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n            # Example processing logic\n            # Add your specific processing logic here\n            processed_data = {\n                \"term\": filename.replace(\".json\", \"\"),\n                \"response\": data\n            }\n\n            # Save the processed data back to the file\n            with open(file_path, 'w', encoding='utf-8') as file:\n                json.dump(processed_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n                concatenated_data.append(data)\n\n    with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n        json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    with open(schema_file_path, 'w', encoding='utf-8') as file:\n        json.dump(schema, file, indent=4, ensure_ascii=False)\n\n    print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n            \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            original_data = entry.get(\"data\", None)\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n\n                        # Check if the revised query exists in the concatenated data\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                        break  # Stop checking other cutoff strings for this query\n\n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                    # Check if the revised query exists in the concatenated data\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n\n            # Write the row to the CSV file\n            csvwriter.writerow([\n                query, revised_query, original_part_of_speech,\n                cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n            ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "30",
        "content": "z0#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # Example API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # API URL with the specified term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to process individual JSON files\ndef process_entries():\n    logging.info(\"Starting process mode.\")\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Processing file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n\n                # Example processing logic\n                processed_data = {\n                    \"term\": filename.replace(\".json\", \"\"),\n                    \"response\": data\n                }\n\n                # Save the processed data back to the file\n                with open(file_path, 'w', encoding='utf-8') as file:\n                    json.dump(processed_data, file, indent=4, ensure_ascii=False)\n                logging.info(f\"Processed and updated file: {file_path}\")\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading or processing file '{file_path}': {e}\")\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n    logging.info(\"Process mode completed.\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    logging.info(\"Starting concatenate mode.\")\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Reading file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n                    concatenated_data.append(data)\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading file '{file_path}': {e}\")\n\n    try:\n        with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n            json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Concatenated data written to: {concatenated_file_path}\")\n        print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing concatenated file '{concatenated_file_path}': {e}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    logging.info(\"Starting schematize mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    try:\n        with open(schema_file_path, 'w', encoding='utf-8') as file:\n            json.dump(schema, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Schema generated and written to: {schema_file_path}\")\n        print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing schema file '{schema_file_path}': {e}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        logging.error(f\"Query Parts of Speech CSV file not found at: {query_parts_of_speech_csv_path}\")\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n            csv_reader = csv.DictReader(file)\n            for row in csv_reader:\n                bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.csv: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading CSV file '{query_parts_of_speech_csv_path}': {e}\")\n        return\n\n    # Open the CSV file for writing\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "61",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in query_to_pos\n            revised_found_in_pos = revised_query in query_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos, original_note_id, revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\", \"Original Note ID\", \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:L{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "55",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Commented out logging setup\n# logging.basicConfig(\n#     filename=log_file_path,\n#     level=logging.DEBUG,\n#     format=\"%(asctime)s - %(levelname)s - %(message)s\",\n#     datefmt=\"%Y-%m-%d %H:%M:%S\"\n# )\n# logging.info(\"Script started\")\n# logging.info(f\"Mode: {mode}\")\n# logging.info(f\"XLSX Output File: {xlsx_output_file}\")\n# logging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\",\n    \"\u0430\u043c\", \"\u044f\u043c\", \"\u0430\u0445\", \"\u044f\u0445\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    # Commented out logging statement\n    # logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        # logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        # logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        # logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n        \n        # Load the Query Parts of Speech JSON file\n        bulgarian_to_pos = {}\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        # logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n            if original_part_of_speech == \"Unknown\":\n                # logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in bulgarian_to_pos\n            revised_found_in_pos = revised_query in bulgarian_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos\n            ])\n\n        # Log unmatched queries\n        if unmatched_revised_queries:\n            # logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n            for qr in unmatched_revised_queries:\n                # logging.warning(f\"Unmatched Revised Query: {qr}\")\n                pass\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:J{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            # logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            # logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        # logging.error(f\"An error occurred during processing: {e}\")\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    # logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "04",
        "content": "# Function to concatenate all JSON files into a single minified JSON file\ndef concatenate_json_files_minified():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a single minified JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, separators=(',', ':'))\n\n    print(f\"Minified concatenated JSON data saved to {concatenated_file_path}\")"
    },
    {
        "version": "19",
        "content": "#!/usr/bin/env python3\n\nimport csv\nimport os\nimport re\nimport json\n\n# Selector to choose the function to run\nmode = \"reconcile\"  # Change this value as needed\n\n# Paths and directories\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nquery_parts_of_speech_csv_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.csv'\ncsv_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/reconciliation_results.csv'\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n            \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            original_data = entry.get(\"data\", None)\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n\n                        # Check if the revised query exists in the concatenated data\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                        break  # Stop checking other cutoff strings for this query\n\n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                    # Check if the revised query exists in the concatenated data\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n\n            # Write the row to the CSV file\n            csvwriter.writerow([\n                query, revised_query, original_part_of_speech,\n                cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n            ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "48",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load concatenated data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # First Pass: Initial processing of queries\n    results = []\n    unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        original_found_in_concatenated = \"No\"\n        original_found_in_pos = \"No\"\n        revised_status = \"Not Attempted\"\n        revised_result = \"Not Attempted\"\n        revised_part_of_speech = \"\"\n        revised_found_in_concatenated = \"No\"\n        revised_found_in_pos = \"No\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n        # Determine \"Found in\" values for original query\n        original_found_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n        original_found_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n\n        # Validate Revised Query\n        if revised_query:\n            revised_found_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n            revised_found_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n            revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n            revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n            revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n            # Log unmatched revised queries\n            if revised_found_in_concatenated == \"No\":\n                unmatched_revised_queries.append(revised_query)\n        else:\n            # Log invalid or empty revised queries\n            revised_status = \"Invalid\"\n            revised_result = \"No Query\"\n\n        results.append([\n            query, original_part_of_speech, original_found_in_concatenated, original_found_in_pos,\n            revised_query, revised_part_of_speech, cutoff_type, cutoff_applied, revised_status, revised_result,\n            revised_found_in_concatenated, revised_found_in_pos\n        ])\n\n    # Log unmatched queries\n    if unmatched_revised_queries:\n        logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n        for qr in unmatched_revised_queries:\n            logging.warning(f\"Unmatched Revised Query: {qr}\")\n\n    # Generate XLSX file\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"\n\n    # Write headers\n    headers = [\n        \"Original Query\", \"Original Part of Speech\", \"Original Found in Concatenated\", \"Original Found in POS\",\n        \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n        \"Revised Status\", \"Revised Result\", \"Revised Found in Concatenated\", \"Revised Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Write data\n    for row in results:\n        sheet.append(row)\n\n    # Auto-fit column widths\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter\n        sheet.column_dimensions[column_letter].width = max_length + 2\n\n    # Apply table style\n    table_range = f\"A1:L{sheet.max_row}\"\n    table = Table(displayName=\"Table1\", ref=table_range)\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n        showRowStripes=True, showColumnStripes=False\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "58",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nimport openpyxl\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"Load the 'Anki' table from the Flashcards.xlsb file.\"\"\"\n    try:\n        import pyxlsb  # pyxlsb is required for reading .xlsb files\n\n        anki_data = []\n        with pyxlsb.open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                headers = next(sheet.rows())\n                header_map = {header.v: idx for idx, header in enumerate(headers)}\n\n                for row in sheet.rows():\n                    row_data = {header: row[idx].v if idx < len(row) else None for header, idx in header_map.items()}\n                    anki_data.append(row_data)\n\n        return anki_data\n\n    except ImportError:\n        print(\"The `pyxlsb` module is required to read .xlsb files. Install it with `pip install pyxlsb`.\")\n        return []\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n        return []\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Create a mapping of queries to parts of speech\n        query_to_pos = {}\n        for row in anki_data:\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip()\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n\n            if bulgarian_1:\n                query_to_pos[bulgarian_1] = part_of_speech\n            if bulgarian_2:\n                query_to_pos[bulgarian_2] = part_of_speech\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in query_to_pos\n            revised_found_in_pos = revised_query in query_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:J{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "09",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\"\nmode = \"schematize\"  # Change this value to \"fetch\", \"process\", \"concatenate\", or \"schematize\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nschema_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/schema.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Function to concatenate all JSON files into a single pretty-printed JSON file\ndef concatenate_json_files_pretty():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a pretty-printed JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Pretty-printed concatenated JSON data saved to {concatenated_file_path}\")\n\n# Function to generate a data dictionary (schema) from the concatenated JSON file\ndef generate_schema():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        data = json.load(file)\n\n    schema = {}\n\n    def explore_structure(obj, path=\"\"):\n        \"\"\"Recursively explore the structure of an object to generate a schema.\"\"\"\n        if isinstance(obj, dict):\n            for key, value in obj.items():\n                current_path = f\"{path}.{key}\" if path else key\n                explore_structure(value, current_path)\n        elif isinstance(obj, list):\n            if path not in schema:\n                schema[path] = {\"type\": \"list\", \"examples\": []}\n            for item in obj[:5]:  # Limit to 5 examples to keep the schema concise\n                explore_structure(item, path)\n        else:\n            if path not in schema:\n                schema[path] = {\"type\": type(obj).__name__, \"examples\": []}\n            if obj not in schema[path][\"examples\"] and len(schema[path][\"examples\"]) < 5:\n                schema[path][\"examples\"].append(obj)\n\n    # Explore the structure of the concatenated JSON data\n    explore_structure(data)\n\n    # Save the schema to a JSON file\n    with open(schema_output_file, 'w', encoding='utf-8') as out_file:\n        json.dump(schema, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Schema generated and saved to {schema_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    confirm = input(\"Fetching data from the API may incur costs. Do you want to continue? (yes/no): \").strip().lower()\n    if confirm == \"yes\":\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            for line in file:\n                query_term = line.strip()\n                if query_term:\n                    fetch_and_save(query_term)\n        print(\"Fetching and saving completed.\")\n    else:\n        print(\"Fetch operation canceled.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize\")"
    },
    {
        "version": "29",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # Example API URL and headers\n    api_url = \"https://example.com/api\"\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for query in queries:\n        try:\n            response = requests.get(f\"{api_url}?query={query}\", headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{query}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for query: {query}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for query '{query}': {e}\")\n        except IOError as e:\n            logging.error(f\"Error writing data for query '{query}' to file: {e}\")\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to process individual JSON files\ndef process_entries():\n    logging.info(\"Starting process mode.\")\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Processing file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n\n                # Example processing logic\n                processed_data = {\n                    \"term\": filename.replace(\".json\", \"\"),\n                    \"response\": data\n                }\n\n                # Save the processed data back to the file\n                with open(file_path, 'w', encoding='utf-8') as file:\n                    json.dump(processed_data, file, indent=4, ensure_ascii=False)\n                logging.info(f\"Processed and updated file: {file_path}\")\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading or processing file '{file_path}': {e}\")\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n    logging.info(\"Process mode completed.\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    logging.info(\"Starting concatenate mode.\")\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Reading file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n                    concatenated_data.append(data)\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading file '{file_path}': {e}\")\n\n    try:\n        with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n            json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Concatenated data written to: {concatenated_file_path}\")\n        print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing concatenated file '{concatenated_file_path}': {e}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    logging.info(\"Starting schematize mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    try:\n        with open(schema_file_path, 'w', encoding='utf-8') as file:\n            json.dump(schema, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Schema generated and written to: {schema_file_path}\")\n        print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing schema file '{schema_file_path}': {e}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        logging.error(f\"Query Parts of Speech CSV file not found at: {query_parts_of_speech_csv_path}\")\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n            csv_reader = csv.DictReader(file)\n            for row in csv_reader:\n                bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.csv: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading CSV file '{query_parts_of_speech_csv_path}': {e}\")\n        return\n\n    # Open the CSV file for writing\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "78",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport json\nfrom datetime import datetime\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    \"\"\"\n    Reconciles entries between Flashcards.xlsb and concatenated.json.\n    Processes 'Bulgarian 1' and 'Bulgarian 2' independently.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated.json\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Prepare reconciliation results\n        results = []\n\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n\n            # Process Bulgarian 1\n            bulgarian_1_status = process_bulgarian_field(bulgarian_1, concatenated_data)\n\n            # Process Bulgarian 2\n            bulgarian_2_status = process_bulgarian_field(bulgarian_2, concatenated_data)\n\n            # Append results\n            results.append([\n                note_id, bulgarian_1, bulgarian_1_status, bulgarian_2, bulgarian_2_status, part_of_speech\n            ])\n\n        # Create the Excel file\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Reconciliation Results\"\n\n        # Write headers\n        headers = [\n            \"Note ID\", \"Bulgarian 1\", \"Bulgarian 1 Status\", \n            \"Bulgarian 2\", \"Bulgarian 2 Status\", \"Part of Speech\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:F{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\ndef process_bulgarian_field(bulgarian_field, concatenated_data):\n    \"\"\"\n    Processes a single Bulgarian field (Bulgarian 1 or Bulgarian 2).\n    Checks its presence and status in concatenated.json.\n    \"\"\"\n    if not bulgarian_field:\n        return \"Missing\"\n\n    # Search for the query in concatenated.json\n    concatenated_entry = next((item for item in concatenated_data if item.get(\"query\") == bulgarian_field), None)\n\n    if not concatenated_entry:\n        return \"Missing\"\n\n    # Check the contents of the data field\n    data = concatenated_entry.get(\"data\", {})\n    if isinstance(data, dict):\n        if data.get(\"error\") == \"Received status code 204\":\n            return \"Not Found\"\n    elif isinstance(data, list):\n        # Check if there are no hits\n        hits = [item.get(\"hits\", []) for item in data]\n        if not any(hits):  # No hits at all\n            return \"No Headword\"\n        # Check for roms in hits\n        if any(\"roms\" in hit for sublist in hits for hit in sublist):\n            return \"Found\"\n\n    return \"No Headword\"\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, reconcile, concatenate\")"
    },
    {
        "version": "68",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nprocessed_file_path = os.path.join(base_directory, \"processed.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef process_entries():\n    \"\"\"\n    Function to process the concatenated.json file into processed.json.\n    This step validates and enriches the data for subsequent reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Write processed data to processed.json\n        with open(processed_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(processed_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Processing completed. Processed data saved to {processed_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during processing: {e}\")\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the processed.json file and the Anki flashcards table.\n    \"\"\"\n    if not os.path.exists(processed_file_path):\n        print(f\"Processed JSON file not found at {processed_file_path}. Please run 'process' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load processed data\n        with open(processed_file_path, 'r', encoding='utf-8') as file:\n            processed_data = json.load(file)\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Yes\" if is_valid else \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile, process, concatenate\")"
    },
    {
        "version": "39",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # Construct the API URL for the term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n            summary_data[f\"Failed API fetch for: {term}\"] += 1\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n            summary_data[f\"File write error for: {term}\"] += 1\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.json: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Open the CSV file for writing with UTF-8 BOM encoding\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n                \"Found in concatenated\", \"Found in POS\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"\"\n                revised_result = \"\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                    summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Determine \"Found in\" values\n                found_in_concatenated = (\n                    \"Original\" if query in concatenated_queries else\n                    \"Revised\" if revised_query in concatenated_queries else\n                    \"Neither\"\n                )\n                found_in_pos = (\n                    \"Original\" if query in bulgarian_to_pos else\n                    \"Revised\" if revised_query in bulgarian_to_pos else\n                    \"Neither\"\n                )\n\n                # Leave \"Revised Status\" and \"Revised Result\" blank if \"Found in POS\" equals \"Original\"\n                if found_in_pos == \"Original\":\n                    revised_status = \"\"\n                    revised_result = \"\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}', Found in concatenated: '{found_in_concatenated}', \"\n                              f\"Found in POS: '{found_in_pos}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n                    found_in_concatenated, found_in_pos\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "28",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"locked\", \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"locked\"  # Default mode is \"locked\" to prevent accidental usage\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function for locked mode\ndef locked_mode():\n    logging.info(\"Locked mode activated. No operations will be performed.\")\n    print(\"The script is in 'locked' mode. No operations will be performed.\")\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Fetch mode is currently disabled.\")\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\n\n# Function to process individual JSON files\ndef process_entries():\n    logging.info(\"Starting process mode.\")\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Processing file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n\n                # Example processing logic\n                processed_data = {\n                    \"term\": filename.replace(\".json\", \"\"),\n                    \"response\": data\n                }\n\n                # Save the processed data back to the file\n                with open(file_path, 'w', encoding='utf-8') as file:\n                    json.dump(processed_data, file, indent=4, ensure_ascii=False)\n                logging.info(f\"Processed and updated file: {file_path}\")\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading or processing file '{file_path}': {e}\")\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n    logging.info(\"Process mode completed.\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    logging.info(\"Starting concatenate mode.\")\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Reading file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n                    concatenated_data.append(data)\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading file '{file_path}': {e}\")\n\n    try:\n        with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n            json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Concatenated data written to: {concatenated_file_path}\")\n        print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing concatenated file '{concatenated_file_path}': {e}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    logging.info(\"Starting schematize mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    try:\n        with open(schema_file_path, 'w', encoding='utf-8') as file:\n            json.dump(schema, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Schema generated and written to: {schema_file_path}\")\n        print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing schema file '{schema_file_path}': {e}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        logging.error(f\"Query Parts of Speech CSV file not found at: {query_parts_of_speech_csv_path}\")\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n            csv_reader = csv.DictReader(file)\n            for row in csv_reader:\n                bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.csv: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading CSV file '{query_parts_of_speech_csv_path}': {e}\")\n        return\n\n    # Open the CSV file for writing\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n# Main workflow\nif mode == \"locked\":\n    locked_mode()\nelif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: locked, fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "79",
        "content": "def process_bulgarian_field(bulgarian_field, concatenated_data):\n    \"\"\"\n    Processes a single Bulgarian field (Bulgarian 1 or Bulgarian 2).\n    Checks its presence and status in concatenated.json.\n\n    \"\"\"\n    if not bulgarian_field:\n        return \"Missing\"\n\n    # Search for the query in concatenated.json\n    concatenated_entry = next((item for item in concatenated_data if item.get(\"query\") == bulgarian_field), None)\n\n    if not concatenated_entry:\n        return \"Missing\"\n\n    # Check the contents of the data field\n    data = concatenated_entry.get(\"data\", {})\n    if isinstance(data, dict):\n        # Explicitly check for the \"Received status code 204\" error\n        if data.get(\"error\") == \"Received status code 204\":\n            return \"Not Found\"\n    elif isinstance(data, list):\n        # Check if there are no hits\n        hits = [item.get(\"hits\", []) for item in data]\n        if not any(hits):  # No hits at all\n            return \"No Headword\"\n        # Check for roms in hits\n        if any(\"roms\" in hit for sublist in hits for hit in sublist):\n            return \"Found\"\n\n    # If none of the above conditions match, assume \"No Headword\"\n    return \"No Headword\""
    },
    {
        "version": "69",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nprocessed_file_path = os.path.join(base_directory, \"processed.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    # This is a placeholder implementation. You should replace it with your actual data-fetching logic.\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef process_entries():\n    \"\"\"\n    Function to process the concatenated.json file into processed.json.\n    This step validates and enriches the data for subsequent reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Write processed data to processed.json\n        with open(processed_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(processed_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Processing completed. Processed data saved to {processed_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during processing: {e}\")\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the processed.json file and the Anki flashcards table.\n    \"\"\"\n    if not os.path.exists(processed_file_path):\n        print(f\"Processed JSON file not found at {processed_file_path}. Please run 'process' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load processed data\n        with open(processed_file_path, 'r', encoding='utf-8') as file:\n            processed_data = json.load(file)\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Yes\" if is_valid else \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, reconcile, concatenate\")"
    },
    {
        "version": "38",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # Construct the API URL for the term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n            summary_data[f\"Failed API fetch for: {term}\"] += 1\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n            summary_data[f\"File write error for: {term}\"] += 1\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.json: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Open the CSV file for writing with UTF-8 BOM encoding\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n                \"Found in concatenated\", \"Found in POS\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                    summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Determine \"Found in\" values\n                found_in_concatenated = (\n                    \"Original\" if query in concatenated_queries else\n                    \"Revised\" if revised_query in concatenated_queries else\n                    \"Neither\"\n                )\n                found_in_pos = (\n                    \"Original\" if query in bulgarian_to_pos else\n                    \"Revised\" if revised_query in bulgarian_to_pos else\n                    \"Neither\"\n                )\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}', Found in concatenated: '{found_in_concatenated}', \"\n                              f\"Found in POS: '{found_in_pos}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n                    found_in_concatenated, found_in_pos\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "18",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json  # <-- Fix: Import the json module\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nquery_parts_of_speech_csv_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.csv'\ncsv_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/reconciliation_results.csv'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return {\n            \"error\": f\"Received status code {response.status_code}\",\n            \"response_text\": response.text\n        }\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)  # <-- Requires the json module\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            data = entry[\"data\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            revised_part_of_speech = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Check for \"Received status code 204\" and matching criteria\n            if isinstance(data, dict) and \"error\" in data and \"Received status code 204\" in data[\"error\"]:\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            # Modify the query by removing the cutoff string\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n\n                            # Re-run the query\n                            revised_data = fetch_and_save(revised_query)\n\n                            # Check if the revised query returned a valid result\n                            if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                                revised_part_of_speech = \"Verb\"\n                            break  # Stop checking other cutoff strings for this query\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                        # Re-run the query\n                        revised_data = fetch_and_save(revised_query)\n\n                        # Check if the revised query matches Part of Speech = Adjective\n                        if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                            revised_part_of_speech = \"Adjective\"\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech, revised_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Received status code 204\", revised_status, revised_result\n                ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "49",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load concatenated data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # First Pass: Initial processing of queries\n    results = []\n    unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        original_found_in_concatenated = \"No\"\n        original_found_in_pos = \"No\"\n        revised_status = \"\"\n        revised_result = \"\"\n        revised_part_of_speech = \"\"\n        revised_found_in_concatenated = \"\"\n        revised_found_in_pos = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n        # Determine \"Found in\" values for original query\n        original_found_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n        original_found_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n\n        # Validate Revised Query\n        if revised_query:\n            revised_found_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n            revised_found_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n            revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n            revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n            revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n            # Log unmatched revised queries\n            if revised_found_in_concatenated == \"No\":\n                unmatched_revised_queries.append(revised_query)\n        else:\n            # Ensure blanks for revised fields if revised query is blank\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_found_in_concatenated = \"\"\n            revised_found_in_pos = \"\"\n\n        results.append([\n            query, original_part_of_speech, original_found_in_concatenated, original_found_in_pos,\n            revised_query, revised_part_of_speech, cutoff_type, cutoff_applied, revised_status, revised_result,\n            revised_found_in_concatenated, revised_found_in_pos\n        ])\n\n    # Log unmatched queries\n    if unmatched_revised_queries:\n        logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n        for qr in unmatched_revised_queries:\n            logging.warning(f\"Unmatched Revised Query: {qr}\")\n\n    # Generate XLSX file\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"\n\n    # Write headers\n    headers = [\n        \"Original Query\", \"Original Part of Speech\", \"Original Found in Concatenated\", \"Original Found in POS\",\n        \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n        \"Revised Status\", \"Revised Result\", \"Revised Found in Concatenated\", \"Revised Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Write data\n    for row in results:\n        sheet.append(row)\n\n    # Auto-fit column widths\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter\n        sheet.column_dimensions[column_letter].width = max_length + 2\n\n    # Apply table style\n    table_range = f\"A1:L{sheet.max_row}\"\n    table = Table(displayName=\"Table1\", ref=table_range)\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n        showRowStripes=True, showColumnStripes=False\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "59",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Create a mapping of queries to parts of speech\n        query_to_pos = {}\n        for row in anki_data:\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n\n            if bulgarian_1:\n                query_to_pos[bulgarian_1] = part_of_speech\n            if bulgarian_2:\n                query_to_pos[bulgarian_2] = part_of_speech\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in query_to_pos\n            revised_found_in_pos = revised_query in query_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:J{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "08",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\"\nmode = \"concatenate\"  # Change this value to \"fetch\" or \"process\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Function to concatenate all JSON files into a single pretty-printed JSON file\ndef concatenate_json_files_pretty():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a pretty-printed JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Pretty-printed concatenated JSON data saved to {concatenated_file_path}\")\n\n# Main workflow\nif mode == \"fetch\":\n    confirm = input(\"Fetching data from the API may incur costs. Do you want to continue? (yes/no): \").strip().lower()\n    if confirm == \"yes\":\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            for line in file:\n                query_term = line.strip()\n                if query_term:\n                    fetch_and_save(query_term)\n        print(\"Fetching and saving completed.\")\n    else:\n        print(\"Fetch operation canceled.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate\")"
    },
    {
        "version": "76",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the concatenated.json file and the Anki flashcards table.\n    This function also processes the concatenated.json file before reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load and process concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Check if the entry is \"Found, no headword\"\n                found_no_headword = False\n                if isinstance(data, list):\n                    for item in data:\n                        hits = item.get(\"hits\", [])\n                        # Check if there is no hit with type \"entry\"\n                        if not any(hit.get(\"type\") == \"entry\" for hit in hits):\n                            found_no_headword = True\n                            break\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid,\n                    \"found_no_headword\": found_no_headword\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate columns\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1 or bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"bulgarian_1\": bulgarian_1,\n                    \"bulgarian_2\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n        query_to_bulgarian_1 = {entry[\"query\"]: entry[\"bulgarian_1\"] for entry in expanded_data}\n        query_to_bulgarian_2 = {entry[\"query\"]: entry[\"bulgarian_2\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n            found_no_headword = entry[\"found_no_headword\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Found, no headword\" if found_no_headword else (\"Yes\" if is_valid else \"No\")\n            revised_found_in_concatenated = \"Found, no headword\" if found_no_headword else \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Bulgarian 1 and Bulgarian 2\n            bulgarian_1 = query_to_bulgarian_1.get(query, \"\")\n            bulgarian_2 = query_to_bulgarian_2.get(query, \"\")\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id, bulgarian_1, bulgarian_2\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\", \"Bulgarian 1\", \"Bulgarian 2\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:P{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, reconcile, concatenate\")"
    },
    {
        "version": "27",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"locked\", \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"locked\"  # Default mode is \"locked\" to prevent accidental usage\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function for locked mode\ndef locked_mode():\n    logging.info(\"Locked mode activated. No operations will be performed.\")\n    print(\"The script is in 'locked' mode. No operations will be performed.\")\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Fetch mode is currently disabled.\")\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\n\n# Function to process individual JSON files\ndef process_entries():\n    logging.info(\"Starting process mode.\")\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Processing file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n\n                # Example processing logic\n                processed_data = {\n                    \"term\": filename.replace(\".json\", \"\"),\n                    \"response\": data\n                }\n\n                # Save the processed data back to the file\n                with open(file_path, 'w', encoding='utf-8') as file:\n                    json.dump(processed_data, file, indent=4, ensure_ascii=False)\n                logging.info(f\"Processed and updated file: {file_path}\")\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading or processing file '{file_path}': {e}\")\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n    logging.info(\"Process mode completed.\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    logging.info(\"Starting concatenate mode.\")\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Reading file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n                    concatenated_data.append(data)\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading file '{file_path}': {e}\")\n\n    try:\n        with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n            json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Concatenated data written to: {concatenated_file_path}\")\n        print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing concatenated file '{concatenated_file_path}': {e}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    logging.info(\"Starting schematize mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    try:\n        with open(schema_file_path, 'w', encoding='utf-8') as file:\n            json.dump(schema, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Schema generated and written to: {schema_file_path}\")\n        print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing schema file '{schema_file_path}': {e}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        logging.error(f\"Query Parts of Speech CSV file not found at: {query_parts_of_speech_csv_path}\")\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n            csv_reader = csv.DictReader(file)\n            for row in csv_reader:\n                bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.info(\"Loaded Query Parts of Speech mappings.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading CSV file '{query_parts_of_speech_csv_path}': {e}\")\n        return\n\n    # Open the CSV file for writing\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n                logging.debug(f\"Processed query '{query}': Revised Query='{revised_query}', Revised Status='{revised_status}'\")\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n# Main workflow\nif mode == \"locked\":\n    locked_mode()\nelif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: locked, fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "13",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nschema_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/schema.json'\nquery_parts_of_speech_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_and_save(term, output_file_path):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_path):\n        print(f\"Query Parts of Speech file not found at {query_parts_of_speech_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech data\n    with open(query_parts_of_speech_path, 'r', encoding='utf-8') as file:\n        query_parts_of_speech = json.load(file)\n\n    # Create a mapping from \"Bulgarian\" field to \"Part of Speech\"\n    bulgarian_to_pos = {}\n    for entry in query_parts_of_speech:\n        # Split by commas if multiple variations exist\n        bulgarian_variations = [variant.strip() for variant in entry[\"Bulgarian\"].split(\",\")]\n        for variation in bulgarian_variations:\n            bulgarian_to_pos[variation] = entry[\"Part of Speech\"]\n\n    # Iterate over the concatenated data to find entries matching the criteria\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        data = entry[\"data\"]\n\n        # Check for \"Received status code 204\" and matching criteria\n        if isinstance(data, dict) and \"error\" in data and \"Received status code 204\" in data[\"error\"]:\n            if query in bulgarian_to_pos and bulgarian_to_pos[query] == \"Verb\":\n                # Check if the query ends with any of the cutoff strings\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n\n                        # Re-run the query\n                        output_file_path = os.path.join(output_directory, f\"{revised_query}.json\")\n                        fetch_and_save(revised_query, output_file_path)\n\n                        # Check if the new query returned a valid result\n                        with open(output_file_path, 'r', encoding='utf-8') as revised_file:\n                            revised_data = json.load(revised_file)\n\n                            if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                                print(f\"Revised query '{revised_query}' for original query '{query}' succeeded.\")\n                            else:\n                                print(f\"Revised query '{revised_query}' for original query '{query}' still failed.\")\n                        break  # Stop checking other cutoff strings for this query\n\n    print(\"Reconciliation completed.\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "42",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.json: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Create a new Excel workbook\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"  # Changed the sheet title to \"Sheet1\"\n\n    # Write the headers\n    headers = [\n        \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n        \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n        \"Found in concatenated\", \"Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Iterate over the concatenated data to find entries matching the criteria\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        revised_status = \"\"\n        revised_result = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                if revised_entry and revised_entry.get(\"data\"):\n                    revised_status = \"Success\"\n                    revised_result = \"Match Found\"\n\n        # Determine \"Found in\" values\n        found_in_concatenated = (\n            \"Original\" if query in concatenated_queries else\n            \"Revised\" if revised_query in concatenated_queries else\n            \"Neither\"\n        )\n        found_in_pos = (\n            \"Original\" if query in bulgarian_to_pos else\n            \"Revised\" if revised_query in bulgarian_to_pos else\n            \"Neither\"\n        )\n\n        # Leave \"Revised Status\" and \"Revised Result\" blank if \"Found in POS\" equals \"Original\"\n        if found_in_pos == \"Original\":\n            revised_status = \"\"\n            revised_result = \"\"\n\n        # Append the row to the worksheet\n        sheet.append([\n            query, revised_query, original_part_of_speech,\n            cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n            found_in_concatenated, found_in_pos\n        ])\n\n    # Auto-fit column widths based on header length\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter  # Get the column letter\n        sheet.column_dimensions[column_letter].width = max_length + 2  # Add padding for readability\n\n    # Create a table in the worksheet\n    table_range = f\"A1:J{sheet.max_row}\"  # Covers all rows including headers\n    table = Table(displayName=\"Table1\", ref=table_range)\n\n    # Apply a table style without banded columns\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\",  # Dark Teal, Table Style Medium 2\n        showFirstColumn=False,\n        showLastColumn=False,\n        showRowStripes=True,  # Keep row stripes\n        showColumnStripes=False  # Turn off banded columns\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook to an XLSX file\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "52",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\",\n    \"\u0430\u043c\", \"\u044f\u043c\", \"\u0430\u0445\", \"\u044f\u0445\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n        \n        # Load the Query Parts of Speech JSON file\n        bulgarian_to_pos = {}\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"No\"\n            original_found_in_pos = \"No\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            revised_found_in_concatenated = \"\"\n            revised_found_in_pos = \"\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n            if original_part_of_speech == \"Unknown\":\n                logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original query\n            original_found_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n            original_found_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_found_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n                revised_found_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n                revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            results.append([\n                query, original_part_of_speech, original_found_in_concatenated, original_found_in_pos,\n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied, revised_status, revised_result,\n                revised_found_in_concatenated, revised_found_in_pos\n            ])\n\n        # Log unmatched queries\n        if unmatched_revised_queries:\n            logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n            for qr in unmatched_revised_queries:\n                logging.warning(f\"Unmatched Revised Query: {qr}\")\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Original Found in Concatenated\", \"Original Found in POS\",\n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n            \"Revised Status\", \"Revised Result\", \"Revised Found in Concatenated\", \"Revised Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:L{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during processing: {e}\")\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "03",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\"\nmode = \"fetch\"  # Change this value to \"process\" or \"concatenate\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Function to concatenate all JSON files into a single file\ndef concatenate_json_files():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a single JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Concatenated JSON data saved to {concatenated_file_path}\")\n\n# Main workflow\nif mode == \"fetch\":\n    with open(input_file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            query_term = line.strip()\n            if query_term:\n                fetch_and_save(query_term)\n    print(\"Fetching and saving completed.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files()\n    print(\"Concatenation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate\")"
    },
    {
        "version": "37",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # Construct the API URL for the term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n            summary_data[f\"Failed API fetch for: {term}\"] += 1\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n            summary_data[f\"File write error for: {term}\"] += 1\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.json: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Open the CSV file for writing with UTF-8 BOM encoding\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n                \"Original Found in concatenated\", \"Revised Found in concatenated\",\n                \"Original Found in POS\", \"Revised Found in POS\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                    summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Check for presence in concatenated.json and POS mappings\n                original_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n                revised_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n                original_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n                revised_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}', Original Found in concatenated: '{original_in_concatenated}', \"\n                              f\"Revised Found in concatenated: '{revised_in_concatenated}', Original Found in POS: '{original_in_pos}', \"\n                              f\"Revised Found in POS: '{revised_in_pos}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n                    original_in_concatenated, revised_in_concatenated,\n                    original_in_pos, revised_in_pos\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "66",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = {}  # Dictionary to track query status in concatenated data\n            for entry in concatenated_data:\n                # Ensure each entry is a dictionary\n                if isinstance(entry, dict):\n                    query = entry.get(\"query\", \"\")\n                    data = entry.get(\"data\", {})\n                    \n                    # Mark as \"No\" if there's an error or empty response_text\n                    if isinstance(data, dict) and (data.get(\"error\") or not data.get(\"response_text\", \"\").strip()):\n                        concatenated_queries[query] = \"No\"\n                    else:\n                        concatenated_queries[query] = \"Yes\"\n                else:\n                    print(f\"Unexpected entry format: {entry}\")\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Determine \"Found in Concatenated\" for the original query\n            original_found_in_concatenated = concatenated_queries.get(query, \"No\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in Concatenated\" and \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = concatenated_queries.get(revised_query, \"No\")\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "46",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load concatenated data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # First Pass: Initial processing of queries\n    results = []\n    revised_queries_to_search = []  # Collect revised queries for the second pass\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        original_found_in_concatenated = \"No\"\n        original_found_in_pos = \"No\"\n        revised_status = \"\"\n        revised_result = \"\"\n        revised_part_of_speech = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    revised_queries_to_search.append(revised_query)\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                revised_queries_to_search.append(revised_query)\n\n        # Determine \"Found in\" values for original query\n        original_found_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n        original_found_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n\n        results.append([\n            query, original_part_of_speech, original_found_in_concatenated, original_found_in_pos,\n            revised_query, revised_part_of_speech, cutoff_type, cutoff_applied, revised_status, revised_result,\n            \"No\", \"No\"  # Revised found in concatenated and POS, will be updated in second pass\n        ])\n\n    # Second Pass: Process revised queries\n    for result in results:\n        revised_query = result[4]  # Revised Query\n        if revised_query:  # Only process if there's a revised query\n            revised_found_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n            revised_found_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n            revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n            revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n            revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n            result[5] = revised_part_of_speech\n            result[8] = revised_status\n            result[9] = revised_result\n            result[10] = revised_found_in_concatenated\n            result[11] = revised_found_in_pos\n\n    # Generate XLSX file\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"\n\n    # Write headers\n    headers = [\n        \"Original Query\", \"Original Part of Speech\", \"Original Found in Concatenated\", \"Original Found in POS\",\n        \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n        \"Revised Status\", \"Revised Result\", \"Revised Found in Concatenated\", \"Revised Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Write data\n    for row in results:\n        sheet.append(row)\n\n    # Auto-fit column widths\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter\n        sheet.column_dimensions[column_letter].width = max_length + 2\n\n    # Apply table style\n    table_range = f\"A1:L{sheet.max_row}\"\n    table = Table(displayName=\"Table1\", ref=table_range)\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n        showRowStripes=True, showColumnStripes=False\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "17",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nquery_parts_of_speech_csv_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.csv'\ncsv_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/reconciliation_results.csv'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return {\n            \"error\": f\"Received status code {response.status_code}\",\n            \"response_text\": response.text\n        }\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            data = entry[\"data\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            revised_part_of_speech = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Check for \"Received status code 204\" and matching criteria\n            if isinstance(data, dict) and \"error\" in data and \"Received status code 204\" in data[\"error\"]:\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            # Modify the query by removing the cutoff string\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n\n                            # Re-run the query\n                            revised_data = fetch_and_save(revised_query)\n\n                            # Check if the revised query returned a valid result\n                            if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                                revised_part_of_speech = \"Verb\"\n                            break  # Stop checking other cutoff strings for this query\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                        # Re-run the query\n                        revised_data = fetch_and_save(revised_query)\n\n                        # Check if the revised query matches Part of Speech = Adjective\n                        if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                            revised_part_of_speech = \"Adjective\"\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech, revised_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Received status code 204\", revised_status, revised_result\n                ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "23",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"fetch\"  # Change this value to \"fetch\", \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_entries():\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")  # Updated behavior for fetch mode\n\n# Function to process individual JSON files\ndef process_entries():\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n            # Example processing logic\n            # Add your specific processing logic here\n            processed_data = {\n                \"term\": filename.replace(\".json\", \"\"),\n                \"response\": data\n            }\n\n            # Save the processed data back to the file\n            with open(file_path, 'w', encoding='utf-8') as file:\n                json.dump(processed_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n                concatenated_data.append(data)\n\n    with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n        json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    with open(schema_file_path, 'w', encoding='utf-8') as file:\n        json.dump(schema, file, indent=4, ensure_ascii=False)\n\n    print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n            \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            original_data = entry.get(\"data\", None)\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n\n                        # Check if the revised query exists in the concatenated data\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                        break  # Stop checking other cutoff strings for this query\n\n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                    # Check if the revised query exists in the concatenated data\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n\n            # Write the row to the CSV file\n            csvwriter.writerow([\n                query, revised_query, original_part_of_speech,\n                cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n            ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "72",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nprocessed_file_path = os.path.join(base_directory, \"processed.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\nschematized_file_path = os.path.join(base_directory, \"schematized.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef process_entries():\n    \"\"\"\n    Function to process the concatenated.json file into processed.json.\n    This step validates and enriches the data for subsequent reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Write processed data to processed.json\n        with open(processed_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(processed_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Processing completed. Processed data saved to {processed_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during processing: {e}\")\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the processed.json file and the Anki flashcards table.\n    \"\"\"\n    if not os.path.exists(processed_file_path):\n        print(f\"Processed JSON file not found at {processed_file_path}. Please run 'process' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load processed data\n        with open(processed_file_path, 'r', encoding='utf-8') as file:\n            processed_data = json.load(file)\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Yes\" if is_valid else \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\ndef schematize_entries():\n    \"\"\"\n    Function to schematize the concatenated.json data into a structured JSON schema.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        schematized_data = []\n        for entry in concatenated_data:\n            query = entry.get(\"query\", \"\")\n            data = entry.get(\"data\", {})\n\n            # Create a structured schema for each entry\n            schematized_entry = {\n                \"query\": query,\n                \"response_text\": data.get(\"response_text\", \"\"),\n                \"error\": data.get(\"error\", None)\n            }\n            schematized_data.append(schematized_entry)\n\n        # Write schematized data to schematized.json\n        with open(schematized_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(schematized_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Schematization completed. Schematized data saved to {schematized_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during schematization: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelif mode == \"schematize\":\n    schematize_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, reconcile, concatenate, schematize\")"
    },
    {
        "version": "62",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in query_to_pos\n            revised_found_in_pos = revised_query in query_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos, revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\", \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:L{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "33",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # Construct the API URL for the term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        logging.error(f\"Query Parts of Speech CSV file not found at: {query_parts_of_speech_csv_path}\")\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n            csv_reader = csv.DictReader(file)\n            for row in csv_reader:\n                bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.csv: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.csv: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading CSV file '{query_parts_of_speech_csv_path}': {e}\")\n        return\n\n    # Open the CSV file for writing\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "07",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\"\nmode = \"fetch\"  # Change this value to \"process\" or \"concatenate\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Function to concatenate all JSON files into a single pretty-printed JSON file\ndef concatenate_json_files_pretty():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a pretty-printed JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Pretty-printed concatenated JSON data saved to {concatenated_file_path}\")\n\n# Main workflow\nif mode == \"fetch\":\n    confirm = input(\"Fetching data from the API may incur costs. Do you want to continue? (yes/no): \").strip().lower()\n    if confirm == \"yes\":\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            for line in file:\n                query_term = line.strip()\n                if query_term:\n                    fetch_and_save(query_term)\n        print(\"Fetching and saving completed.\")\n    else:\n        print(\"Fetch operation canceled.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate\")"
    },
    {
        "version": "56",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\",\n    \"\u0430\u0445\", \"\u044f\u0445\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the Query Parts of Speech JSON file\n        bulgarian_to_pos = {}\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in bulgarian_to_pos\n            revised_found_in_pos = revised_query in bulgarian_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:J{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "47",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load concatenated data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # First Pass: Initial processing of queries\n    results = []\n    unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        original_found_in_concatenated = \"No\"\n        original_found_in_pos = \"No\"\n        revised_status = \"Not Attempted\"\n        revised_result = \"Not Attempted\"\n        revised_part_of_speech = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n        # Determine \"Found in\" values for original query\n        original_found_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n        original_found_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n\n        # Validate Revised Query\n        if revised_query:\n            revised_found_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n            revised_found_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n            revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n            revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n            revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n            # Log unmatched revised queries\n            if revised_found_in_concatenated == \"No\":\n                unmatched_revised_queries.append(revised_query)\n        else:\n            # Log invalid or empty revised queries\n            revised_status = \"Invalid\"\n            revised_result = \"No Query\"\n\n        results.append([\n            query, original_part_of_speech, original_found_in_concatenated, original_found_in_pos,\n            revised_query, revised_part_of_speech, cutoff_type, cutoff_applied, revised_status, revised_result,\n            revised_found_in_concatenated, revised_found_in_pos\n        ])\n\n    # Log unmatched queries\n    if unmatched_revised_queries:\n        logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n        for qr in unmatched_revised_queries:\n            logging.warning(f\"Unmatched Revised Query: {qr}\")\n\n    # Generate XLSX file\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"\n\n    # Write headers\n    headers = [\n        \"Original Query\", \"Original Part of Speech\", \"Original Found in Concatenated\", \"Original Found in POS\",\n        \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n        \"Revised Status\", \"Revised Result\", \"Revised Found in Concatenated\", \"Revised Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Write data\n    for row in results:\n        sheet.append(row)\n\n    # Auto-fit column widths\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter\n        sheet.column_dimensions[column_letter].width = max_length + 2\n\n    # Apply table style\n    table_range = f\"A1:L{sheet.max_row}\"\n    table = Table(displayName=\"Table1\", ref=table_range)\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n        showRowStripes=True, showColumnStripes=False\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "16",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\nimport csv\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nquery_parts_of_speech_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.json'\ncsv_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/reconciliation_results.csv'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return {\n            \"error\": f\"Received status code {response.status_code}\",\n            \"response_text\": response.text\n        }\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_path):\n        print(f\"Query Parts of Speech file not found at {query_parts_of_speech_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech data\n    with open(query_parts_of_speech_path, 'r', encoding='utf-8') as file:\n        query_parts_of_speech = json.load(file)\n\n    # Extract JSONdata array\n    query_parts_of_speech_data = query_parts_of_speech.get(\"JSONdata\", [])\n    bulgarian_to_pos = {entry[\"Bulgarian\"]: entry[\"Part of Speech\"] for entry in query_parts_of_speech_data}\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            data = entry[\"data\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            revised_part_of_speech = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Check for \"Received status code 204\" and matching criteria\n            if isinstance(data, dict) and \"error\" in data and \"Received status code 204\" in data[\"error\"]:\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            # Modify the query by removing the cutoff string\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n\n                            # Re-run the query\n                            revised_data = fetch_and_save(revised_query)\n\n                            # Check if the revised query returned a valid result\n                            if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                                revised_part_of_speech = \"Verb\"\n                            break  # Stop checking other cutoff strings for this query\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                        # Re-run the query\n                        revised_data = fetch_and_save(revised_query)\n\n                        # Check if the revised query matches Part of Speech = Adjective\n                        if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                            revised_part_of_speech = \"Adjective\"\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech, revised_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Received status code 204\", revised_status, revised_result\n                ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "80",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport json\nfrom datetime import datetime\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    \"\"\"\n    Reconciles entries between Flashcards.xlsb and concatenated.json.\n    Processes 'Bulgarian 1' and 'Bulgarian 2' independently.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated.json\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Prepare reconciliation results\n        results = []\n\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n\n            # Process Bulgarian 1\n            bulgarian_1_status = process_bulgarian_field(bulgarian_1, concatenated_data)\n\n            # Process Bulgarian 2\n            bulgarian_2_status = process_bulgarian_field(bulgarian_2, concatenated_data)\n\n            # Append results\n            results.append([\n                note_id, bulgarian_1, bulgarian_1_status, bulgarian_2, bulgarian_2_status, part_of_speech\n            ])\n\n        # Create the Excel file\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Reconciliation Results\"\n\n        # Write headers\n        headers = [\n            \"Note ID\", \"Bulgarian 1\", \"Bulgarian 1 Status\", \n            \"Bulgarian 2\", \"Bulgarian 2 Status\", \"Part of Speech\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:F{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\ndef process_bulgarian_field(bulgarian_field, concatenated_data):\n    \"\"\"\n    Processes a single Bulgarian field (Bulgarian 1 or Bulgarian 2).\n    Checks its presence and status in concatenated.json.\n    \"\"\"\n    if not bulgarian_field:\n        return \"Missing\"\n\n    # Search for the query in concatenated.json\n    concatenated_entry = next((item for item in concatenated_data if item.get(\"query\") == bulgarian_field), None)\n\n    if not concatenated_entry:\n        return \"Missing\"\n\n    # Check the contents of the data field\n    data = concatenated_entry.get(\"data\", {})\n    if isinstance(data, dict):\n        # Explicitly check for the \"Received status code 204\" error\n        if data.get(\"error\") == \"Received status code 204\":\n            return \"Not Found\"\n    elif isinstance(data, list):\n        # Check if there are no hits\n        hits = [item.get(\"hits\", []) for item in data]\n        if not any(hits):  # No hits at all\n            return \"No Headword\"\n        # Check for roms in hits\n        if any(\"roms\" in hit for sublist in hits for hit in sublist):\n            return \"Found\"\n\n    # If none of the above conditions match, assume \"No Headword\"\n    return \"No Headword\"\n\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "22",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"fetch\"  # Change this value to \"fetch\", \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\ncsv_output_file = os.path.join(base_directory, \"reconciliation_results.csv\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_entries():\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")  # Updated behavior for fetch mode\n\n# Function to process individual JSON files\ndef process_entries():\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n            # Example processing logic\n            # Add your specific processing logic here\n            processed_data = {\n                \"term\": filename.replace(\".json\", \"\"),\n                \"response\": data\n            }\n\n            # Save the processed data back to the file\n            with open(file_path, 'w', encoding='utf-8') as file:\n                json.dump(processed_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n                concatenated_data.append(data)\n\n    with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n        json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n\n    print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    with open(schema_file_path, 'w', encoding='utf-8') as file:\n        json.dump(schema, file, indent=4, ensure_ascii=False)\n\n    print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n            \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            original_data = entry.get(\"data\", None)\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        # Modify the query by removing the cutoff string\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n\n                        # Check if the revised query exists in the concatenated data\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                        break  # Stop checking other cutoff strings for this query\n\n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    # Modify the query by replacing \"\u043d\u043e\" with \"\u0435\u043d\"\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n\n                    # Check if the revised query exists in the concatenated data\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n\n            # Write the row to the CSV file\n            csvwriter.writerow([\n                query, revised_query, original_part_of_speech,\n                cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n            ])\n\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "73",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\nschematized_file_path = os.path.join(base_directory, \"schematized.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the concatenated.json file and the Anki flashcards table.\n    This function also processes the concatenated.json file before reconciliation.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load and process concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        processed_data = []\n        for entry in concatenated_data:\n            # Ensure each entry is a dictionary\n            if isinstance(entry, dict):\n                query = entry.get(\"query\", \"\")\n                data = entry.get(\"data\", {})\n\n                # Check if the entry is \"Example Only\"\n                is_example_only = False\n                if isinstance(data, list):\n                    for item in data:\n                        hits = item.get(\"hits\", [])\n                        if any('<span class=\"example\">' in hit.get(\"source\", \"\") for hit in hits):\n                            is_example_only = True\n                            break\n\n                # Validate entry\n                is_valid = isinstance(data, dict) and not data.get(\"error\") and data.get(\"response_text\", \"\").strip()\n                processed_data.append({\n                    \"query\": query,\n                    \"is_valid\": is_valid,\n                    \"is_example_only\": is_example_only\n                })\n            else:\n                print(f\"Unexpected entry format: {entry}\")\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in processed_data:\n            query = entry[\"query\"]\n            is_valid = entry[\"is_valid\"]\n            is_example_only = entry[\"is_example_only\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"Example Only\" if is_example_only else (\"Yes\" if is_valid else \"No\")\n            revised_found_in_concatenated = \"Example Only\" if is_example_only else \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = \"No\"\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\ndef schematize_entries():\n    \"\"\"\n    Function to schematize the concatenated.json data into a structured JSON schema.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        schematized_data = []\n        for entry in concatenated_data:\n            query = entry.get(\"query\", \"\")\n            data = entry.get(\"data\", {})\n\n            # Create a structured schema for each entry\n            schematized_entry = {\n                \"query\": query,\n                \"response_text\": data.get(\"response_text\", \"\"),\n                \"error\": data.get(\"error\", None)\n            }\n            schematized_data.append(schematized_entry)\n\n        # Write schematized data to schematized.json\n        with open(schematized_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(schematized_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Schematization completed. Schematized data saved to {schematized_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during schematization: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelif mode == \"schematize\":\n    schematize_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, reconcile, concatenate, schematize\")"
    },
    {
        "version": "63",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"No\"\n            revised_found_in_concatenated = \"No\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"No\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            if query in concatenated_queries:\n                original_found_in_concatenated = \"Yes\"\n            if revised_query in concatenated_queries:\n                revised_found_in_concatenated = \"Yes\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n            if revised_query in query_to_pos:\n                revised_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "32",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # Construct the API URL for the term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to process individual JSON files\ndef process_entries():\n    logging.info(\"Starting process mode.\")\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Processing file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n\n                # Example processing logic\n                processed_data = {\n                    \"term\": filename.replace(\".json\", \"\"),\n                    \"response\": data\n                }\n\n                # Save the processed data back to the file\n                with open(file_path, 'w', encoding='utf-8') as file:\n                    json.dump(processed_data, file, indent=4, ensure_ascii=False)\n                logging.info(f\"Processed and updated file: {file_path}\")\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading or processing file '{file_path}': {e}\")\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n    logging.info(\"Process mode completed.\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    logging.info(\"Starting concatenate mode.\")\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            try:\n                logging.debug(f\"Reading file: {file_path}\")\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    data = json.load(file)\n                    concatenated_data.append(data)\n            except (IOError, UnicodeDecodeError) as e:\n                logging.error(f\"Error reading file '{file_path}': {e}\")\n\n    try:\n        with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n            json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Concatenated data written to: {concatenated_file_path}\")\n        print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing concatenated file '{concatenated_file_path}': {e}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    logging.info(\"Starting schematize mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    try:\n        with open(schema_file_path, 'w', encoding='utf-8') as file:\n            json.dump(schema, file, indent=4, ensure_ascii=False)\n        logging.info(f\"Schema generated and written to: {schema_file_path}\")\n        print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n    except IOError as e:\n        logging.error(f\"Error writing schema file '{schema_file_path}': {e}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        logging.error(f\"Query Parts of Speech CSV file not found at: {query_parts_of_speech_csv_path}\")\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n            csv_reader = csv.DictReader(file)\n            for row in csv_reader:\n                bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.csv: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading CSV file '{query_parts_of_speech_csv_path}': {e}\")\n        return\n\n    # Open the CSV file for writing\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "06",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\"\nmode = \"fetch\"  # Change this value to \"process\" or \"concatenate\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Function to concatenate all JSON files into a single pretty-printed JSON file\ndef concatenate_json_files_pretty():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a pretty-printed JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Pretty-printed concatenated JSON data saved to {concatenated_file_path}\")\n\n# Main workflow\nif mode == \"fetch\":\n    with open(input_file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            query_term = line.strip()\n            if query_term:\n                fetch_and_save(query_term)\n    print(\"Fetching and saving completed.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate\")"
    },
    {
        "version": "57",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        \n        # Load the Query Parts of Speech JSON file\n        bulgarian_to_pos = {}\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"Neither\"\n            found_in_pos = \"Neither\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original and revised queries\n            original_found_in_concatenated = query in concatenated_queries\n            revised_found_in_concatenated = revised_query in concatenated_queries if revised_query else False\n            original_found_in_pos = query in bulgarian_to_pos\n            revised_found_in_pos = revised_query in bulgarian_to_pos if revised_query else False\n\n            # Consolidate \"Found in Concatenated\"\n            if original_found_in_concatenated and revised_found_in_concatenated:\n                found_in_concatenated = \"Both\"\n            elif original_found_in_concatenated:\n                found_in_concatenated = \"Original\"\n            elif revised_found_in_concatenated:\n                found_in_concatenated = \"Revised\"\n\n            # Consolidate \"Found in POS\"\n            if original_found_in_pos and revised_found_in_pos:\n                found_in_pos = \"Both\"\n            elif original_found_in_pos:\n                found_in_pos = \"Original\"\n            elif revised_found_in_pos:\n                found_in_pos = \"Revised\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos else \"No Match\"\n\n                # Log unmatched revised queries\n                if not revised_found_in_concatenated:\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                query, original_part_of_speech, revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, found_in_concatenated, found_in_pos\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Revised Query\", \"Revised Part of Speech\",\n            \"Cutoff Type\", \"Cutoff Applied\", \"Revised Status\", \"Revised Result\",\n            \"Found in Concatenated\", \"Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:J{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "77",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# File paths\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef fetch_data():\n    \"\"\"\n    Function to fetch raw data and save it to concatenated.json.\n    This function can fetch data from an API, database, or other sources.\n    \"\"\"\n    try:\n        # Example data to simulate fetching\n        fetched_data = [\n            {\"query\": \"\u043a\u043e\u0440\u043c\u043e\u0440\u0430\u043d\", \"data\": {\"response_text\": \"Some response\", \"error\": None}},\n            {\"query\": \"\u0441\u043b\u043e\u043d\", \"data\": {\"response_text\": \"\", \"error\": \"Received status code 204\"}},\n            {\"query\": \"\u0442\u0438\u0433\u044a\u0440\", \"data\": {\"response_text\": \"Another response\", \"error\": None}}\n        ]\n\n        # Save fetched data to concatenated.json\n        with open(concatenated_file_path, 'w', encoding='utf-8') as outfile:\n            json.dump(fetched_data, outfile, ensure_ascii=False, indent=4)\n\n        print(f\"Data fetching completed. Fetched data saved to {concatenated_file_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred during data fetching: {e}\")\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef reconcile_entries():\n    \"\"\"\n    Reconciles entries between the concatenated.json file and the Anki flashcards table.\n    Processes both 'Bulgarian 1' and 'Bulgarian 2' independently.\n    \"\"\"\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'fetch' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n\n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                    \"source\": \"Bulgarian 1\"\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                    \"source\": \"Bulgarian 2\"\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n        query_to_source = {entry[\"query\"]: entry[\"source\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in expanded_data:\n            query = entry[\"query\"]\n            source = entry[\"source\"]\n            note_id = entry[\"note_id\"]\n            part_of_speech = entry[\"part_of_speech\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            found_in_concatenated = \"No\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            revised_note_id = \"\"\n\n            # Check if query exists in concatenated data\n            concatenated_entry = next((item for item in concatenated_data if item.get(\"query\") == query), None)\n            if concatenated_entry:\n                found_in_concatenated = \"Yes\"\n                data = concatenated_entry.get(\"data\", {})\n                if isinstance(data, list):\n                    # Check if no hit has type \"entry\"\n                    if not any(hit.get(\"type\") == \"entry\" for item in data for hit in item.get(\"hits\", [])):\n                        found_in_concatenated = \"Found, no headword\"\n\n            # Handle Verbs with cutoff logic\n            if part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_pos = \"Yes\" if revised_query in query_to_pos else \"No\"\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_status = \"Success\" if revised_found_in_pos == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n                if revised_found_in_pos == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                note_id, query, part_of_speech, source,\n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                found_in_concatenated, original_found_in_pos, revised_found_in_pos,\n                revised_status, revised_result, revised_note_id\n            ])\n\n        # Create the Excel file\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Reconciliation Results\"\n\n        # Write headers\n        headers = [\n            \"Note ID\", \"Query\", \"Part of Speech\", \"Source\",\n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n            \"Found in Concatenated\", \"Original Found in POS\", \"Revised Found in POS\",\n            \"Revised Status\", \"Revised Result\", \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_data()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, reconcile, concatenate\")"
    },
    {
        "version": "26",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\n\n# Selector to choose the function to run\n# Options: \"locked\", \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"locked\"  # Default mode is \"locked\" to prevent accidental usage\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_csv_path = os.path.join(base_directory, \"Query Parts of Speech.csv\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Function for locked mode\ndef locked_mode():\n    logging.info(\"Locked mode activated. No operations will be performed.\")\n    print(\"The script is in 'locked' mode. No operations will be performed.\")\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Fetch mode is currently disabled.\")\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\n\n# Function to process individual JSON files\ndef process_entries():\n    logging.info(\"Starting process mode.\")\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            logging.debug(f\"Processing file: {file_path}\")\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n            # Example processing logic\n            processed_data = {\n                \"term\": filename.replace(\".json\", \"\"),\n                \"response\": data\n            }\n\n            # Save the processed data back to the file\n            with open(file_path, 'w', encoding='utf-8') as file:\n                json.dump(processed_data, file, indent=4, ensure_ascii=False)\n            logging.info(f\"Processed and updated file: {file_path}\")\n\n    print(f\"Processing completed. Files updated in {output_directory}\")\n    logging.info(\"Process mode completed.\")\n\n# Function to concatenate JSON files into a single file\ndef concatenate_entries():\n    logging.info(\"Starting concatenate mode.\")\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            logging.debug(f\"Reading file: {file_path}\")\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n                concatenated_data.append(data)\n\n    with open(concatenated_file_path, 'w', encoding='utf-8') as file:\n        json.dump(concatenated_data, file, indent=4, ensure_ascii=False)\n    logging.info(f\"Concatenated data written to: {concatenated_file_path}\")\n    print(f\"Concatenation completed. Results saved to {concatenated_file_path}\")\n\n# Function to generate a schema from concatenated JSON\ndef generate_schema():\n    logging.info(\"Starting schematize mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Example schema generation logic\n    schema = {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n\n    for entry in concatenated_data:\n        for key, value in entry.items():\n            if key not in schema[\"items\"][\"properties\"]:\n                schema[\"items\"][\"properties\"][key] = {\"type\": type(value).__name__}\n\n    with open(schema_file_path, 'w', encoding='utf-8') as file:\n        json.dump(schema, file, indent=4, ensure_ascii=False)\n    logging.info(f\"Schema generated and written to: {schema_file_path}\")\n    print(f\"Schema generation completed. Results saved to {schema_file_path}\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_csv_path):\n        logging.error(f\"Query Parts of Speech CSV file not found at: {query_parts_of_speech_csv_path}\")\n        print(f\"Query Parts of Speech CSV file not found at {query_parts_of_speech_csv_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech CSV file\n    bulgarian_to_pos = {}\n    with open(query_parts_of_speech_csv_path, 'r', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            bulgarian_word = row.get(\"Bulgarian\", \"\").strip()\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip()\n            if bulgarian_word and part_of_speech:\n                bulgarian_to_pos[bulgarian_word] = part_of_speech\n    logging.info(\"Loaded Query Parts of Speech mappings.\")\n\n    # Open the CSV file for writing\n    with open(csv_output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csvwriter = csv.writer(csvfile)\n\n        # Write the header row\n        csvwriter.writerow([\n            \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n            \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n        ])\n\n        # Iterate over the concatenated data to find entries matching the criteria\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n            original_data = entry.get(\"data\", None)\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            revised_status = \"Failure\"\n            revised_result = \"No Match\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n            if original_part_of_speech == \"Unknown\":\n                logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n                        break\n\n            # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n\n            # Write the row to the CSV file\n            csvwriter.writerow([\n                query, revised_query, original_part_of_speech,\n                cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n            ])\n            logging.debug(f\"Processed query '{query}': Revised Query='{revised_query}', Revised Status='{revised_status}'\")\n\n    logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n\n# Main workflow\nif mode == \"locked\":\n    locked_mode()\nelif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: locked, fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "12",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Change this value to \"process\", \"concatenate\", \"schematize\", or \"reconcile\" as needed\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\nschema_output_file = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/schema.json'\nquery_parts_of_speech_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Query Parts of Speech.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch data from the API\ndef fetch_and_save(term, output_file_path):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_path):\n        print(f\"Query Parts of Speech file not found at {query_parts_of_speech_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n        concatenated_data = json.load(file)\n\n    # Load the Query Parts of Speech data\n    with open(query_parts_of_speech_path, 'r', encoding='utf-8') as file:\n        query_parts_of_speech = json.load(file)\n\n    # Create a mapping from \"Bulgarian\" field to \"Part of Speech\"\n    bulgarian_to_pos = {}\n    for entry in query_parts_of_speech:\n        # Split by commas if multiple variations exist\n        bulgarian_variations = [variant.strip() for variant in entry[\"Bulgarian\"].split(\",\")]\n        for variation in bulgarian_variations:\n            bulgarian_to_pos[variation] = entry[\"Part of Speech\"]\n\n    # Iterate over the concatenated data to find entries matching the criteria\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        data = entry[\"data\"]\n\n        # Check for \"Received status code 204\" and matching criteria\n        if isinstance(data, dict) and \"error\" in data and \"Received status code 204\" in data[\"error\"]:\n            if query in bulgarian_to_pos and bulgarian_to_pos[query] == \"Verb\":\n                if query.endswith(\" \u0441\u0435\") or query.endswith(\" (\u0441\u0435)\"):\n                    # Modify the query by removing the \" \u0441\u0435\" or \" (\u0441\u0435)\"\n                    revised_query = re.sub(r\"( \u0441\u0435| \\(\u0441\u0435\\))$\", \"\", query)\n\n                    # Re-run the query\n                    output_file_path = os.path.join(output_directory, f\"{revised_query}.json\")\n                    fetch_and_save(revised_query, output_file_path)\n\n                    # Check if the new query returned a valid result\n                    with open(output_file_path, 'r', encoding='utf-8') as revised_file:\n                        revised_data = json.load(revised_file)\n\n                        if not (isinstance(revised_data, dict) and \"error\" in revised_data):\n                            print(f\"Revised query '{revised_query}' for original query '{query}' succeeded.\")\n                        else:\n                            print(f\"Revised query '{revised_query}' for original query '{query}' still failed.\")\n\n    print(\"Reconciliation completed.\")\n\n# Main workflow\nif mode == \"fetch\":\n    print(\"The 'fetch' mode is currently disabled to prevent accidental usage.\")\nelif mode == \"process\":\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            process_json_file(os.path.join(output_directory, filename))\n    print(\"Processing completed.\")\nelif mode == \"concatenate\":\n    concatenate_json_files_pretty()\n    print(\"Concatenation completed.\")\nelif mode == \"schematize\":\n    generate_schema()\n    print(\"Schematization completed.\")\nelif mode == \"reconcile\":\n    reconcile_entries()\n    print(\"Reconciliation completed.\")\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "43",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Verify the existence of required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Create a new Excel workbook\n    workbook = Workbook()\n    sheet = workbook.active\n    sheet.title = \"Sheet1\"  # Set the sheet title to \"Sheet1\"\n\n    # Write the headers\n    headers = [\n        \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n        \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\",\n        \"Found in concatenated\", \"Found in POS\"\n    ]\n    sheet.append(headers)\n\n    # Iterate over the concatenated data to find entries matching the criteria\n    for entry in concatenated_data:\n        query = entry[\"query\"]\n        original_data = entry.get(\"data\", None)\n\n        # Default values for fields\n        revised_query = \"\"\n        cutoff_type = \"\"\n        cutoff_applied = \"\"\n        revised_status = \"\"\n        revised_result = \"\"\n\n        # Part of Speech\n        original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n        if original_part_of_speech == \"Unknown\":\n            logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n            summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n        # Handle Verbs with cutoff logic\n        if original_part_of_speech == \"Verb\":\n            for cutoff in cutoff_strings:\n                if query.endswith(cutoff):\n                    revised_query = query[: -len(cutoff)].strip()\n                    cutoff_type = \"Verb Cutoff\"\n                    cutoff_applied = cutoff\n                    revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                    if revised_entry and revised_entry.get(\"data\"):\n                        revised_status = \"Success\"\n                        revised_result = \"Match Found\"\n                    break\n\n        # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n        elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n            if query.endswith(\"\u043d\u043e\"):\n                revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                cutoff_type = \"Adverb Modification\"\n                cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                if revised_entry and revised_entry.get(\"data\"):\n                    revised_status = \"Success\"\n                    revised_result = \"Match Found\"\n\n        # Determine \"Found in\" values\n        found_in_concatenated = (\n            \"Original\" if query in concatenated_queries else\n            \"Revised\" if revised_query in concatenated_queries else\n            \"Neither\"\n        )\n        found_in_pos = (\n            \"Original\" if query in bulgarian_to_pos else\n            \"Revised\" if revised_query in bulgarian_to_pos else\n            \"Neither\"\n        )\n\n        # Leave \"Revised Status\" and \"Revised Result\" blank if \"Found in POS\" equals \"Original\"\n        if found_in_pos == \"Original\":\n            revised_status = \"\"\n            revised_result = \"\"\n\n        # Append the row to the worksheet\n        sheet.append([\n            query, revised_query, original_part_of_speech,\n            cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result,\n            found_in_concatenated, found_in_pos\n        ])\n\n    # Auto-fit column widths based on header length and content\n    for column in sheet.columns:\n        max_length = max(len(str(cell.value)) for cell in column if cell.value)\n        column_letter = column[0].column_letter  # Get the column letter\n        sheet.column_dimensions[column_letter].width = max_length + 2  # Add padding for readability\n\n    # Create a table in the worksheet\n    table_range = f\"A1:J{sheet.max_row}\"  # Covers all rows including headers\n    table = Table(displayName=\"Table1\", ref=table_range)\n\n    # Apply a table style without banded columns\n    style = TableStyleInfo(\n        name=\"TableStyleMedium2\",  # Dark Teal, Table Style Medium 2\n        showFirstColumn=False,\n        showLastColumn=False,\n        showRowStripes=True,  # Keep row stripes\n        showColumnStripes=False  # Turn off banded columns\n    )\n    table.tableStyleInfo = style\n    sheet.add_table(table)\n\n    # Save the workbook to an XLSX file\n    try:\n        workbook.save(xlsx_output_file)\n        logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "53",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"XLSX Output File: {xlsx_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\",\n    \"\u0430\u043c\", \"\u044f\u043c\", \"\u0430\u0445\", \"\u044f\u0445\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    \n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = set(entry[\"query\"] for entry in concatenated_data)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n        \n        # Load the Query Parts of Speech JSON file\n        bulgarian_to_pos = {}\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"No\"\n            original_found_in_pos = \"No\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            revised_found_in_concatenated = \"\"\n            revised_found_in_pos = \"\"\n            either_found_in_concatenated = \"No\"\n            either_found_in_pos = \"No\"\n\n            # Part of Speech\n            original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n            if original_part_of_speech == \"Unknown\":\n                logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in\" values for original query\n            original_found_in_concatenated = \"Yes\" if query in concatenated_queries else \"No\"\n            original_found_in_pos = \"Yes\" if query in bulgarian_to_pos else \"No\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_found_in_concatenated = \"Yes\" if revised_query in concatenated_queries else \"No\"\n                revised_found_in_pos = \"Yes\" if revised_query in bulgarian_to_pos else \"No\"\n                revised_part_of_speech = bulgarian_to_pos.get(revised_query, \"Unknown\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            # Determine \"Either Found\" values\n            either_found_in_concatenated = \"Yes\" if original_found_in_concatenated == \"Yes\" or revised_found_in_concatenated == \"Yes\" else \"No\"\n            either_found_in_pos = \"Yes\" if original_found_in_pos == \"Yes\" or revised_found_in_pos == \"Yes\" else \"No\"\n\n            results.append([\n                query, original_part_of_speech, original_found_in_concatenated, original_found_in_pos,\n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied, revised_status, revised_result,\n                revised_found_in_concatenated, revised_found_in_pos, either_found_in_concatenated, either_found_in_pos\n            ])\n\n        # Log unmatched queries\n        if unmatched_revised_queries:\n            logging.warning(f\"Unmatched Revised Queries: {len(unmatched_revised_queries)}\")\n            for qr in unmatched_revised_queries:\n                logging.warning(f\"Unmatched Revised Query: {qr}\")\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Query\", \"Original Part of Speech\", \"Original Found in Concatenated\", \"Original Found in POS\",\n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \"Cutoff Applied\",\n            \"Revised Status\", \"Revised Result\", \"Revised Found in Concatenated\", \"Revised Found in POS\",\n            \"Either Found in Concatenated\", \"Either Found in POS\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            logging.info(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            logging.error(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred during processing: {e}\")\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile\")"
    },
    {
        "version": "02",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport json\nimport os\nimport re\nimport sys\n\n# Paths and directories\ninput_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/Inputs_for_PONS_API.txt'\noutput_directory = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/PONS json Files'\nconcatenated_file_path = '/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning/concatenated.json'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to fetch and save raw API response\ndef fetch_and_save(term):\n    url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n    headers = {\n        \"X-Secret\": \"XXX\"\n    }\n    response = requests.get(url, headers=headers)\n\n    # Create a JSON file for each term\n    output_file_path = os.path.join(output_directory, f\"{term}.json\")\n    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n        if response.status_code == 200:\n            json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n        else:\n            error_message = {\n                \"error\": f\"Received status code {response.status_code}\",\n                \"response_text\": response.text\n            }\n            json.dump(error_message, json_file, ensure_ascii=False, indent=4)\n\n# Function to extract and process data from a JSON file\ndef process_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n\n        # Check for errors in the JSON file\n        if \"error\" in data:\n            print(f\"Error in file {file_path}: {data['error']}\")\n            return\n\n        # Example processing logic\n        # Extract headwords and translations from the JSON\n        processed_data = []\n        for language_entry in data:\n            for hit in language_entry.get('hits', []):\n                for rom in hit.get('roms', []):\n                    headword = rom.get('headword', 'Unknown')\n                    translations = [\n                        translation.get('target', 'Unknown')\n                        for arab in rom.get('arabs', [])\n                        for translation in arab.get('translations', [])\n                    ]\n                    processed_data.append({\n                        \"headword\": headword,\n                        \"translations\": translations\n                    })\n\n        # Print the processed data\n        print(json.dumps(processed_data, ensure_ascii=False, indent=4))\n\n# Function to concatenate all JSON files into a single file\ndef concatenate_json_files():\n    concatenated_data = []\n\n    for filename in os.listdir(output_directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(output_directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                # Add the filename (minus .json) as a top-level field\n                data_with_query = {\n                    \"query\": filename[:-5],  # Remove \".json\"\n                    \"data\": data\n                }\n                concatenated_data.append(data_with_query)\n\n    # Write the concatenated data to a single JSON file\n    with open(concatenated_file_path, 'w', encoding='utf-8') as out_file:\n        json.dump(concatenated_data, out_file, ensure_ascii=False, indent=4)\n\n    print(f\"Concatenated JSON data saved to {concatenated_file_path}\")\n\n# Main workflow selector\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python3 PONSAPI.py <mode>\")\n        print(\"Modes:\")\n        print(\"  fetch       - Fetch and save data from the API\")\n        print(\"  process     - Process saved JSON files\")\n        print(\"  concatenate - Concatenate all JSON files into a single file\")\n        sys.exit(1)\n\n    mode = sys.argv[1].lower()\n\n    if mode == \"fetch\":\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            for line in file:\n                query_term = line.strip()\n                if query_term:\n                    fetch_and_save(query_term)\n        print(\"Fetching and saving completed.\")\n    elif mode == \"process\":\n        for filename in os.listdir(output_directory):\n            if filename.endswith('.json'):\n                process_json_file(os.path.join(output_directory, filename))\n        print(\"Processing completed.\")\n    elif mode == \"concatenate\":\n        concatenate_json_files()\n    else:\n        print(f\"Unknown mode: {mode}\")\n        print(\"Available modes: fetch, process, concatenate\")\n        sys.exit(1)"
    },
    {
        "version": "36",
        "content": "#!/usr/bin/env python3\n\nimport requests\nimport csv\nimport os\nimport re\nimport json\nimport logging\nfrom datetime import datetime\nfrom collections import Counter\n\n# Selector to choose the function to run\n# Options: \"fetch\", \"process\", \"concatenate\", \"schematize\", \"reconcile\"\nmode = \"reconcile\"  # Default mode is set to \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\ninput_file_path = os.path.join(base_directory, \"Inputs_for_PONS_API.txt\")\noutput_directory = os.path.join(base_directory, \"PONS json Files\")\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nschema_file_path = os.path.join(base_directory, \"schema.json\")\nquery_parts_of_speech_json_path = os.path.join(base_directory, \"Query Parts of Speech.json\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\ncsv_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.csv\")\nlog_file_path = os.path.join(base_directory, f\"debug_{timestamp}.log\")\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Setup logging\nlogging.basicConfig(\n    filename=log_file_path,\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogging.info(\"Script started\")\nlogging.info(f\"Mode: {mode}\")\nlogging.info(f\"CSV Output File: {csv_output_file}\")\nlogging.info(f\"Log File: {log_file_path}\")\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n# Function to fetch data from the API\ndef fetch_entries():\n    logging.info(\"Starting fetch mode.\")\n\n    # API headers\n    headers = {\n        \"X-Secret\": \"XXX\",  # Secret header\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Read queries from the input file\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as file:\n            queries = [line.strip() for line in file.readlines()]\n        logging.debug(f\"Loaded {len(queries)} queries from input file.\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading input file '{input_file_path}': {e}\")\n        return\n\n    # Fetch data for each query\n    for term in queries:\n        try:\n            # Construct the API URL for the term\n            url = f\"https://api.pons.com/v1/dictionary?q={term}&l=bgen\"\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = response.json()\n\n            # Save the response to a JSON file\n            output_file_path = os.path.join(output_directory, f\"{term}.json\")\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                json.dump(data, output_file, indent=4, ensure_ascii=False)\n            logging.info(f\"Fetched and saved data for term: {term}\")\n        except requests.RequestException as e:\n            logging.error(f\"Error fetching data for term '{term}': {e}\")\n            summary_data[f\"Failed API fetch for: {term}\"] += 1\n        except IOError as e:\n            logging.error(f\"Error writing data for term '{term}' to file: {e}\")\n            summary_data[f\"File write error for: {term}\"] += 1\n\n    logging.info(\"Fetch mode completed.\")\n\n# Function to reconcile specific cases in the concatenated JSON\ndef reconcile_entries():\n    logging.info(\"Starting reconcile mode.\")\n    if not os.path.exists(concatenated_file_path):\n        logging.error(f\"Concatenated JSON file not found at: {concatenated_file_path}\")\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(query_parts_of_speech_json_path):\n        logging.error(f\"Query Parts of Speech JSON file not found at: {query_parts_of_speech_json_path}\")\n        print(f\"Query Parts of Speech JSON file not found at {query_parts_of_speech_json_path}. Please ensure it exists.\")\n        return\n\n    # Load the concatenated JSON data\n    try:\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n        logging.debug(f\"Total queries in concatenated.json: {len(concatenated_data)}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading concatenated file '{concatenated_file_path}': {e}\")\n        return\n\n    # Load the Query Parts of Speech JSON file\n    bulgarian_to_pos = {}\n    try:\n        with open(query_parts_of_speech_json_path, 'r', encoding='utf-8') as file:\n            parts_of_speech_data = json.load(file).get(\"JSONdata\", [])\n            for entry in parts_of_speech_data:\n                bulgarian_word = entry.get(\"Bulgarian\", \"\").strip()\n                part_of_speech = entry.get(\"Part of Speech\", \"\").strip()\n                if bulgarian_word and part_of_speech:\n                    bulgarian_to_pos[bulgarian_word] = part_of_speech\n        logging.debug(f\"Total mappings in Query Parts of Speech.json: {len(bulgarian_to_pos)}\")\n        logging.debug(f\"Sample mappings from Query Parts of Speech.json: {list(bulgarian_to_pos.items())[:5]}\")\n    except (IOError, UnicodeDecodeError) as e:\n        logging.error(f\"Error reading JSON file '{query_parts_of_speech_json_path}': {e}\")\n        return\n\n    # Open the CSV file for writing with UTF-8 BOM encoding\n    try:\n        with open(csv_output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow([\n                \"Original Query\", \"Revised Query\", \"Original Part of Speech\", \"Cutoff Type\",\n                \"Cutoff Applied\", \"Original Status\", \"Revised Status\", \"Revised Result\"\n            ])\n\n            # Iterate over the concatenated data to find entries matching the criteria\n            for entry in concatenated_data:\n                query = entry[\"query\"]\n                original_data = entry.get(\"data\", None)\n\n                # Default values for fields\n                revised_query = \"\"\n                cutoff_type = \"\"\n                cutoff_applied = \"\"\n                revised_status = \"Failure\"\n                revised_result = \"No Match\"\n\n                # Part of Speech\n                original_part_of_speech = bulgarian_to_pos.get(query, \"Unknown\")\n                if original_part_of_speech == \"Unknown\":\n                    logging.warning(f\"Part of Speech for query '{query}' is unknown.\")\n                    summary_data[f\"Unknown Part of Speech: {query}\"] += 1\n\n                # Handle Verbs with cutoff logic\n                if original_part_of_speech == \"Verb\":\n                    for cutoff in cutoff_strings:\n                        if query.endswith(cutoff):\n                            revised_query = query[: -len(cutoff)].strip()\n                            cutoff_type = \"Verb Cutoff\"\n                            cutoff_applied = cutoff\n                            revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                            if revised_entry and revised_entry.get(\"data\"):\n                                revised_status = \"Success\"\n                                revised_result = \"Match Found\"\n                            break\n\n                # Handle Adverbs and Unclassified Words ending in \"\u043d\u043e\"\n                elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                    if query.endswith(\"\u043d\u043e\"):\n                        revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                        cutoff_type = \"Adverb Modification\"\n                        cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                        revised_entry = next((e for e in concatenated_data if e[\"query\"] == revised_query), None)\n                        if revised_entry and revised_entry.get(\"data\"):\n                            revised_status = \"Success\"\n                            revised_result = \"Match Found\"\n\n                # Log details for each query\n                logging.debug(f\"Original Query: '{query}', Revised Query: '{revised_query}', \"\n                              f\"Original Part of Speech: '{original_part_of_speech}', Cutoff Applied: '{cutoff_applied}', \"\n                              f\"Revised Status: '{revised_status}'\")\n\n                # Write the row to the CSV file\n                csvwriter.writerow([\n                    query, revised_query, original_part_of_speech,\n                    cutoff_type, cutoff_applied, \"Processed\", revised_status, revised_result\n                ])\n\n        logging.info(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n        print(f\"Reconciliation completed. Results saved to {csv_output_file}\")\n    except IOError as e:\n        logging.error(f\"Error writing reconciliation results to '{csv_output_file}': {e}\")\n\n    # Generate and log summary\n    logging.info(\"\\n--- Summary ---\")\n    for key, count in summary_data.items():\n        logging.info(f\"{key}: {count}\")\n\n# Main workflow\nif mode == \"fetch\":\n    fetch_entries()\nelif mode == \"process\":\n    process_entries()\nelif mode == \"concatenate\":\n    concatenate_entries()\nelif mode == \"schematize\":\n    generate_schema()\nelif mode == \"reconcile\":\n    reconcile_entries()\nelse:\n    logging.error(f\"Unknown mode: {mode}\")\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: fetch, process, concatenate, schematize, reconcile\")"
    },
    {
        "version": "67",
        "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\nfrom collections import Counter\nfrom openpyxl import Workbook\nfrom openpyxl.worksheet.table import Table, TableStyleInfo\nfrom pyxlsb import open_workbook\n\n# Selector to choose the function to run\nmode = \"reconcile\"\n\n# Base directory for all file paths\nbase_directory = \"/Users/phobrla/Library/CloudStorage/OneDrive-Personal/Documents/Bulgarian Language Learning\"\n\n# Paths and directories\nconcatenated_file_path = os.path.join(base_directory, \"concatenated.json\")\nflashcards_xlsb_path = os.path.join(base_directory, \"Flashcards.xlsb\")\n\n# Generate the timestamp suffix\ntimestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\nxlsx_output_file = os.path.join(base_directory, f\"reconciliation_results_{timestamp}.xlsx\")\n\n# Ensure the output directory exists\nos.makedirs(base_directory, exist_ok=True)\n\n# Strings to be cut off during the re-search\ncutoff_strings = [\n    \" \u0441\u0435\", \" [\u0432]\", \" \u0441\u0438\", \" [\u0441]\", \" \u0437\u0430\", \" \u0432\", \" (\u0441\u0435)\", \" (\u0441\u0435) [\u0441]\", \" (\u0441\u0435) \u0434\u0430\"\n]\n\n# Summary data for logging\nsummary_data = Counter()\n\n\ndef load_anki_table_from_xlsb(file_path):\n    \"\"\"\n    Load the 'Anki' table from the Flashcards.xlsb file in read-only mode.\n    This function can parse the file even if it is open in Microsoft Excel.\n    \"\"\"\n    anki_data = []\n\n    try:\n        with open_workbook(file_path) as workbook:\n            with workbook.get_sheet(\"Anki\") as sheet:\n                rows = sheet.rows()\n                headers = [cell.v for cell in next(rows)]  # Extract headers from the first row\n                header_map = {header: idx for idx, header in enumerate(headers)}\n\n                for row in rows:\n                    row_data = {header: row[header_map[header]].v if header_map[header] < len(row) else None for header in headers}\n                    anki_data.append(row_data)\n\n    except Exception as e:\n        print(f\"An error occurred while reading the 'Anki' table: {e}\")\n    \n    return anki_data\n\n\ndef concatenate_files():\n    \"\"\"\n    Function to concatenate multiple files into a single JSON file.\n    This function is a placeholder and should be implemented if needed.\n    \"\"\"\n    print(\"Concatenate functionality is not implemented yet.\")\n    # Implement concatenation logic here if needed\n\n\ndef reconcile_entries():\n    \"\"\"\n    Function to reconcile entries between the concatenated JSON file and the Anki flashcards table.\n    \"\"\"\n    # Load required files\n    if not os.path.exists(concatenated_file_path):\n        print(f\"Concatenated JSON file not found at {concatenated_file_path}. Please run 'concatenate' mode first.\")\n        return\n\n    if not os.path.exists(flashcards_xlsb_path):\n        print(f\"Flashcards.xlsb file not found at {flashcards_xlsb_path}. Please ensure it exists.\")\n        return\n\n    try:\n        # Load concatenated data\n        with open(concatenated_file_path, 'r', encoding='utf-8') as file:\n            concatenated_data = json.load(file)\n            concatenated_queries = {}  # Dictionary to track query status in concatenated data\n            for entry in concatenated_data:\n                # Ensure each entry is a dictionary\n                if isinstance(entry, dict):\n                    query = entry.get(\"query\", \"\")\n                    data = entry.get(\"data\", {})\n                    \n                    # Mark as \"No\" if there's an error or empty response_text\n                    if isinstance(data, dict) and (data.get(\"error\") or not data.get(\"response_text\", \"\").strip()):\n                        concatenated_queries[query] = \"No\"\n                    else:\n                        concatenated_queries[query] = \"Yes\"\n                else:\n                    print(f\"Unexpected entry format: {entry}\")\n        \n        # Load the \"Anki\" table from Flashcards.xlsb\n        anki_data = load_anki_table_from_xlsb(flashcards_xlsb_path)\n\n        # Process both Bulgarian 1 and Bulgarian 2 as separate queries\n        expanded_data = []\n        for row in anki_data:\n            note_id = row.get(\"Note ID\", \"\").strip() if row.get(\"Note ID\") else \"\"\n            part_of_speech = row.get(\"Part of Speech\", \"\").strip() if row.get(\"Part of Speech\") else \"\"\n            bulgarian_1 = row.get(\"Bulgarian 1\", \"\").strip() if row.get(\"Bulgarian 1\") else \"\"\n            bulgarian_2 = row.get(\"Bulgarian 2\", \"\").strip() if row.get(\"Bulgarian 2\") else \"\"\n\n            if bulgarian_1:\n                expanded_data.append({\n                    \"query\": bulgarian_1,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n            if bulgarian_2:\n                expanded_data.append({\n                    \"query\": bulgarian_2,\n                    \"note_id\": note_id,\n                    \"part_of_speech\": part_of_speech,\n                })\n\n        # Create mappings for queries\n        query_to_pos = {entry[\"query\"]: entry[\"part_of_speech\"] for entry in expanded_data}\n        query_to_note_id = {entry[\"query\"]: entry[\"note_id\"] for entry in expanded_data}\n\n        # Process data\n        results = []\n        unmatched_revised_queries = []  # Collect unmatched revised queries for debugging\n\n        for entry in concatenated_data:\n            query = entry[\"query\"]\n\n            # Default values for fields\n            revised_query = \"\"\n            cutoff_type = \"\"\n            cutoff_applied = \"\"\n            original_found_in_concatenated = \"No\"\n            revised_found_in_concatenated = \"\"\n            original_found_in_pos = \"No\"\n            revised_found_in_pos = \"\"\n            revised_status = \"\"\n            revised_result = \"\"\n            revised_part_of_speech = \"\"\n            original_note_id = \"\"\n            revised_note_id = \"\"\n\n            # Part of Speech\n            original_part_of_speech = query_to_pos.get(query, \"Unknown\")\n            original_note_id = query_to_note_id.get(query, \"\")\n\n            # Determine \"Found in Concatenated\" for the original query\n            original_found_in_concatenated = concatenated_queries.get(query, \"No\")\n\n            # Handle Verbs with cutoff logic\n            if original_part_of_speech == \"Verb\":\n                for cutoff in cutoff_strings:\n                    if query.endswith(cutoff):\n                        revised_query = query[: -len(cutoff)].strip()\n                        cutoff_type = \"Verb Cutoff\"\n                        cutoff_applied = cutoff\n                        break\n\n            # Handle Adverbs and Unclassified Words\n            elif original_part_of_speech in [\"Adverb\", \"Unclassified Word\"]:\n                if query.endswith(\"\u0435\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u0435\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u0435\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043d\u043e\"):\n                    revised_query = re.sub(r\"\u043d\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043d\u043e \u2192 \u0435\u043d\"\n                elif query.endswith(\"\u043e\"):\n                    revised_query = re.sub(r\"\u043e$\", \"\u0435\u043d\", query)\n                    cutoff_type = \"Adverb Modification\"\n                    cutoff_applied = \"\u043e \u2192 \u0435\u043d\"\n\n            # Determine \"Found in Concatenated\" and \"Found in POS\" for revised query\n            if revised_query:\n                revised_found_in_concatenated = concatenated_queries.get(revised_query, \"No\")\n                if revised_query in query_to_pos:\n                    revised_found_in_pos = \"Yes\"\n            else:\n                revised_found_in_concatenated = \"\"\n                revised_found_in_pos = \"\"\n\n            if query in query_to_pos:\n                original_found_in_pos = \"Yes\"\n\n            # Validate Revised Query\n            if revised_query:\n                revised_part_of_speech = query_to_pos.get(revised_query, \"Unknown\")\n                revised_note_id = query_to_note_id.get(revised_query, \"\")\n                revised_status = \"Success\" if revised_found_in_concatenated == \"Yes\" else \"Failure\"\n                revised_result = \"Match Found\" if revised_found_in_pos == \"Yes\" else \"No Match\"\n\n                # Log unmatched revised queries\n                if revised_found_in_concatenated == \"No\":\n                    unmatched_revised_queries.append(revised_query)\n            else:\n                # Ensure blanks for revised fields if revised query is blank\n                revised_status = \"\"\n                revised_result = \"\"\n\n            results.append([\n                original_note_id, query, original_part_of_speech, \n                revised_query, revised_part_of_speech, cutoff_type, cutoff_applied,\n                revised_status, revised_result, \n                original_found_in_concatenated, revised_found_in_concatenated, \n                original_found_in_pos, revised_found_in_pos, \n                revised_note_id\n            ])\n\n        # Create the Excel file only after all processing is complete\n        workbook = Workbook()\n        sheet = workbook.active\n        sheet.title = \"Sheet1\"\n\n        # Write headers\n        headers = [\n            \"Original Note ID\", \"Original Query\", \"Original Part of Speech\", \n            \"Revised Query\", \"Revised Part of Speech\", \"Cutoff Type\", \n            \"Cutoff Applied\", \"Revised Status\", \"Revised Result\", \n            \"Original Found in Concatenated\", \"Revised Found in Concatenated\", \n            \"Original Found in POS\", \"Revised Found in POS\", \n            \"Revised Note ID\"\n        ]\n        sheet.append(headers)\n\n        # Write data\n        for row in results:\n            sheet.append(row)\n\n        # Auto-fit column widths\n        for column in sheet.columns:\n            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n            column_letter = column[0].column_letter\n            sheet.column_dimensions[column_letter].width = max_length + 2\n\n        # Apply table style\n        table_range = f\"A1:N{sheet.max_row}\"\n        table = Table(displayName=\"Table1\", ref=table_range)\n        style = TableStyleInfo(\n            name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False,\n            showRowStripes=True, showColumnStripes=False\n        )\n        table.tableStyleInfo = style\n        sheet.add_table(table)\n\n        # Save the workbook\n        try:\n            workbook.save(xlsx_output_file)\n            print(f\"Reconciliation completed. Results saved to {xlsx_output_file}\")\n        except IOError as e:\n            print(f\"Error writing reconciliation results to '{xlsx_output_file}': {e}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check the log for details.\")\n\n\n# Main workflow\nif mode == \"reconcile\":\n    reconcile_entries()\nelif mode == \"concatenate\":\n    concatenate_files()\nelse:\n    print(f\"Unknown mode: {mode}\")\n    print(\"Available modes: reconcile, concatenate\")"
    }
]